if(message_packet->access_id == 373844)
	{
		printf("***L1 id %d 373844 write block cycle %llu\n", cache->id, P_TIME);
		/*STOP;*/
	}


if(message_packet->access_id == 373844)
			{
				printf("***l2 id %d 373844 hit on retry cycle %llu***\n", cache->id, P_TIME);
				assert(message_packet->access_type = cgm_access_put_clnx);
				/*STOP;*/
			}

//on a write back with inclusive caches L2 Merges the line
	//if the write back is a surprise the block will be exclusive in the L2 cache, but the data is old.

	//WB from L1 D cache
	if(cache->lastl2_caches[my_pid].last_queue == l2_caches[my_pid].Rx_queue_top)
	{

		//get the state of the cache block
		cache_get_block_status(&(l2_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

		switch(*cache_block_state_ptr)
		{
			case cgm_cache_block_noncoherent:
			case cgm_cache_block_owned:
			fatal("l2_cache_ctrl(): Invalid block state on writeback as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
				break;

			case cgm_cache_block_invalid:
			/*star todo fix this, right now our test set up just randomly
			writes to blocks each time we access l2, this is one reason why the WB could miss
			come back to this once we have finished the rest of the protocol and don't need to set l2 inf*/

			/*printf("l2 wb miss cycle %llu\n", P_TIME);*/
			P_PAUSE(l2_caches[my_pid].latency);

			message_packet = list_remove(l2_caches[my_pid].last_queue, message_packet);
			/*assert(message_packet->flush_pending == 1);*/
			packet_destroy(message_packet);
			break;

			//star todo missing L2 cache code to upgrade its own block from shared to exclusive


			//star todo check the shared state here?
			case cgm_cache_block_shared:
			case cgm_cache_block_modified:
			case cgm_cache_block_exclusive:

				if(*cache_block_state_ptr == cgm_cache_block_shared)
				{
					printf("L2 WB received with block in shared state\n");
				}

				//set modified if the line was exclusive
				if(*cache_block_state_ptr == cgm_cache_block_exclusive)
				{
					cgm_cache_set_block_state(&(l2_caches[my_pid]), message_packet->set, message_packet->way, cgm_cache_block_modified);
				}

				//in the real world we would merge changes with L2 block here.

				//move on
				P_PAUSE(l2_caches[my_pid].latency);

				message_packet = list_remove(l2_caches[my_pid].last_queue, message_packet);
				packet_destroy(message_packet);
				break;
		}
	}
	//if here the L2 generated it's own write back.
	else if(l2_caches[my_pid].last_queue == l2_caches[my_pid].write_back_buffer)
	{
		/*star todo figure out a better way to deal with the pending flush state
		maybe look into the scheduler*/

		//if zero the flush has finished.
		if(message_packet->flush_pending == 0)
		{

			/*printf(" not waiting\n");*/

			P_PAUSE(l2_caches[my_pid].latency);

			l3_map = cgm_l3_cache_map(message_packet->set);
			message_packet->l2_cache_id = l2_caches[my_pid].id;
			message_packet->l2_cache_name = str_map_value(&l2_strn_map, l2_caches[my_pid].id);

			message_packet->src_name = l2_caches[my_pid].name;
			message_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);
			message_packet->dest_name = l3_caches[l3_map].name;
			message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

			//send the write back to the L3 cache.
			cache_put_io_down_queue(&(l2_caches[my_pid]), message_packet);

			/*write backs are internally scheduled so decrement the counter
			figure out a way to do this better..
			perhaps have the cache advance itself when the evict results in a write back buffer entry.*/
		}
		//still waiting so run again or find something else to do.
		else if (message_packet->flush_pending == 1 || message_packet->downgrade_pending == 1)
		{
			//NOTE: flush may have been sent prior to recieving a get_fwd and a subsequent downgrade
			//wait on either case.

			//do nothing for now.
			printf("here\n");
			getchar();
		}
		else
		{
			fatal("l2 cache invalid flush_pending bit on write back packet\n");
		}

		//run again.
		step--;
	}


	return;

	/*//set the state to exclusive and clear the transient state
	cgm_cache_set_block_state(cache, message_packet->set, message_packet->way, cgm_cache_block_exclusive);
	cgm_cache_set_block_transient_state(cache, message_packet->set, message_packet->way, (int) NULL, cgm_cache_block_null);

	//enter the retry state
	message_packet->access_type = cgm_cache_get_retry_state(message_packet->cpu_access_type);
	assert(message_packet->access_type == cgm_access_store_retry);
	assert(message_packet->coalesced != 1);

	message_packet = list_remove(cache->last_queue, message_packet);
	list_enqueue(cache->retry_queue, message_packet);*/

void cgm_mesi_l2_getx_fwd(struct cache_t *cache, struct cgm_packet_t *message_packet){

	int cache_block_hit;
	int cache_block_state;
	int *cache_block_hit_ptr = &cache_block_hit;
	int *cache_block_state_ptr = &cache_block_state;

	struct cgm_packet_t *getx_fwd_reply_packet;
	struct cgm_packet_t *pending_getx_fwd_request;

	struct cgm_packet_t *inval_packet;

	int l3_map;

	//star todo add in the wb buffer checks
	/*printf("L2 id %d getx fwd received from L2 id %d cycle %llu\n", cache->id, message_packet->l2_cache_id, P_TIME);*/

	cache_get_block_status(cache, message_packet, cache_block_hit_ptr, cache_block_state_ptr);

	/*printf("L2 id %d block hit %d as %s\n", cache->id, *cache_block_hit_ptr, str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));*/

	//block still present in L2 cache
	if(*cache_block_hit_ptr == 1)
	{
		//a GET_FWD means the block is exclusive in this core, but could also be modified
		assert(*cache_block_state_ptr == cgm_cache_block_exclusive || *cache_block_state_ptr == cgm_cache_block_modified);

		/*if not found uh-oh...*/
		/*the address better be the same too...*/
		/*assert(pending_getx_fwd_request);
		assert(pending_getx_fwd_request->address == message_packet->address);*/

		//invalidate the local block
		cgm_cache_set_block(cache, message_packet->set, message_packet->way, message_packet->tag, cgm_cache_block_invalid);

		//prepare to forward the block
		//set access type
		message_packet->access_type = cgm_access_put_clnx;

		//set the block state
		message_packet->cache_block_state = cgm_cache_block_exclusive;

		//set message package size
		message_packet->size = l2_caches[str_map_string(&node_strn_map, message_packet->l2_cache_name)].block_size;

		//fwd block to requesting core
		//update routing headers swap dest and src
		//requesting node
		message_packet->dest_name = str_map_value(&node_strn_map, message_packet->src_id);
		message_packet->dest_id = str_map_string(&node_strn_map, message_packet->src_name);

		//owning node L2
		message_packet->src_name = cache->name;
		message_packet->src_id = str_map_string(&node_strn_map, cache->name);

		/*printf("requester name %s and id %d\n", pending_request->src_name, pending_request->l2_cache_id);*/
		/*temp_id = pending_request->access_id;*/

		//transmit block to requesting node
		message_packet = list_remove(cache->last_queue, message_packet);
		list_enqueue(cache->Tx_queue_bottom, message_packet);
		advance(cache->cache_io_down_ec);
		/*printf("L2 id %d exclusive block forwarded to L2 cache id %d\n", cache->id, message_packet->l2_cache_id);*/


		///////////////
		//getx_fwd_ack
		///////////////

		//send the getx_fwd_ack to L3 cache.

		//create downgrade_ack
		getx_fwd_reply_packet = packet_create();
		assert(getx_fwd_reply_packet);

		init_getx_fwd_ack_packet(getx_fwd_reply_packet, message_packet->address);

		//fwd reply (getx_fwd_ack) to L3
		l3_map = cgm_l3_cache_map(message_packet->set);

		//fakes src as the requester
		/*reply_packet->l2_cache_id = l2_caches[my_pid].id;*/
		getx_fwd_reply_packet->l2_cache_id = message_packet->l2_cache_id;
		getx_fwd_reply_packet->l2_cache_name = message_packet->src_name;

		getx_fwd_reply_packet->src_name = cache->name;
		getx_fwd_reply_packet->src_id = str_map_string(&node_strn_map, cache->name);
		getx_fwd_reply_packet->dest_name = l3_caches[l3_map].name;
		getx_fwd_reply_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

		//transmit downgrad_ack to L3 (home)
		list_enqueue(cache->Tx_queue_bottom, getx_fwd_reply_packet);
		advance(cache->cache_io_down_ec);

		//destroy the L1 D inval_ack message because we don't need it anymore.
		/*message_packet = list_remove(cache->last_queue, message_packet);
		free(message_packet);*/

		/*printf("L2 id %d sent getx_fwd_ack to L3 id %d cycle %llu\n", cache->id, l3_map, P_TIME);*/
	}
	//block was evicted while flush was in progress
	else if(*cache_block_hit_ptr == 0)
	{
		//block was locally dropped

		//set cgm_access_getx_fwd_nack
		message_packet->access_type = cgm_access_getx_fwd_nack;

		//fwd reply (downgrade_nack) to L3
		l3_map = cgm_l3_cache_map(message_packet->set);

		/*here send the nack down to the L3
		don't change any of the source information*/

		/*message_packet->l2_cache_id = l2_caches[my_pid].id;
		message_packet->l2_cache_name = str_map_value(&l2_strn_map, l2_caches[my_pid].id);
		reply_packet->src_name = l2_caches[my_pid].name;
		reply_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);*/


		message_packet->dest_name = l3_caches[l3_map].name;
		message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

		//charge delay
		P_PAUSE(cache->latency);

		//transmit block to L3
		cache_put_io_down_queue(cache, message_packet);
	}
	else
	{
		fatal("cgm_mesi_l2_getx_fwd(): l2 miss on getx_fwd check this\n");
	}


	return;

//set retry state
		message_packet->access_type = cgm_cache_get_retry_state(message_packet->cpu_access_type);

//check if directory entry is dirty or clean for victim
		//dirty = cgm_cache_get_dir_dirty_bit(cache, message_packet->set, message_packet->l3_victim_way);

		/*victim_state = cgm_cache_get_block_state(cache, message_packet->set, message_packet->l3_victim_way);

		first if the block is modified it is dirty and needs to be written back
		move a copy of the block to the write back buffer
		if (victim_state == cgm_cache_block_modified)
		{
			//move the block to the WB buffer
			struct cgm_packet_t *write_back_packet = packet_create();

			//star todo remember to set l2 cache id in WB packet
			init_write_back_packet(cache, write_back_packet, message_packet->set, message_packet->l3_victim_way);

			list_enqueue(cache->write_back_buffer, write_back_packet);
		}*/

		/*//clear the directory entry
		cgm_cache_clear_dir(cache, message_packet->set, message_packet->l3_victim_way);*/
		
		//set the sharer in the directory
		/*cgm_cache_set_dir(cache, message_packet->set, message_packet->l3_victim_way, message_packet->l2_cache_id);*/

		/*assert(message_packet->cache_block_state);*/
		

/*enum cgm_cache_block_state_t{

	cgm_cache_block_invalid = 0,
	cgm_cache_block_noncoherent,
	cgm_cache_block_modified,
	cgm_cache_block_owned,
	cgm_cache_block_exclusive,
	cgm_cache_block_shared,
	cgm_cache_block_transient,
	cgm_cache_block_null,
	cgm_cache_block_state_num
};*/

/*enum protocol_kind_t {
	cgm_protocol_mesi = 0,
	cgm_protocol_moesi,
	cgm_protocol_gmesi,
	cgm_protocol_non_coherent,
	num_cgm_protocol_types
};

enum cgm_access_kind_t {
	cgm_access_invalid = 0,
	cgm_access_fetch,
	cgm_access_load,
	cgm_access_store,
	cgm_access_nc_store,
	cgm_access_nc_load,
	cgm_access_store_v,
	cgm_access_load_s,
	cgm_access_load_v,
	cgm_access_prefetch,
	cgm_access_gets, //get shared
	cgm_access_gets_i,
	cgm_access_get, //get specific to d caches
	cgm_access_get_fwd,
	cgm_access_gets_s, //get shared specific to s caches
	cgm_access_gets_v, //get shared specific to v caches
	cgm_access_getx, //get exclusive (or get with intent to write)
	cgm_access_inv,  //invalidation request
	cgm_access_inv_ack,
	cgm_access_upgrade, //upgrade request
	cgm_access_upgrade_ack,
	cgm_access_downgrade, //downgrade request
	cgm_access_downgrade_ack,
	cgm_access_downgrade_nack,
	cgm_access_mc_get,	//request sent to system agent/memory controller
	cgm_access_mc_put,	//reply from system agent/memory controller
	cgm_access_put_clnx, //put block in exclusive or modified state
	cgm_access_putx, //request for write back of cache block exclusive data.
	cgm_access_puts, //request for write back of cache block in shared state.
	cgm_access_puto, //request for write back of cache block in owned state.
	cgm_access_puto_shared, //request for write back of cache block in owned state but other sharers of the block exist.
	cgm_access_unblock, //message to unblock next cache level/directory for blocking protocols.
	cgm_access_retry,
	cgm_access_fetch_retry,
	cgm_access_load_retry,
	cgm_access_store_retry,
	cgm_access_write_back,
	cgm_access_retry_i,//not used
	num_access_types
};*/

/*struct cgm_packet_t{

	char *name;

	//star todo clean this up when the simulator is done.
	enum cgm_access_kind_t access_type;
	enum cgm_access_kind_t l1_access_type;
	enum cgm_access_kind_t cpu_access_type;
	enum cgm_access_kind_t gpu_access_type;

	int l1_cache_id;
	char *l2_cache_name;
	int l2_cache_id;
	int gpu_cache_id;

	//access data
	long long access_id;
	unsigned int address;
	int set;
	int tag;
	int way;
	unsigned int offset;
	int size;
	int coalesced;

	//for evictions, write backs, downgrades
	int flush_pending;
	int downgrade;
	int downgrade_pending;
	int downgrade_ack;
	int inval;
	int inval_pending;
	int inval_ack;


	int l1_victim_way;
	int l2_victim_way;
	int l3_victim_way;

	//for protocol messages
	enum cgm_cache_block_state_t cache_block_state;

	//for routing
	char *src_name;
	int src_id;
	char *dest_name;
	int dest_id;

	//for m2s CPU and GPU
	struct linked_list_t *event_queue;
	int *witness_ptr;
	void *data;

	//stats
	long long start_cycle;
	long long end_cycle;
};

struct cgm_packet_status_t{

	//used for global memory list
	enum cgm_access_kind_t access_type;
	unsigned int address;
	long long access_id;
	int in_flight;
};*/

/*struct cgm_packet_status_t;*/


/*#include <cgm/cache.h>*/
	
	/*enum cache_type_enum{

	l1_i_cache_t,
	l1_d_cache_t,
	l2_cache_t,
	l3_cache_t,
	gpu_s_cache_t,
	gpu_v_cache_t,
	gpu_l2_cache_t
};*/

/*enum cache_waylist_enum{

	cache_waylist_head,
	cache_waylist_tail
};*/

/*enum cache_policy_t{

	cache_policy_invalid = 0,
	cache_policy_lru,
	cache_policy_fifo,
	cache_policy_random,
	cache_policy_num
};*/

/*struct cache_block_t{

	struct cache_block_t *way_next;
	struct cache_block_t *way_prev;

	int tag;
	int set;
	int transient_tag;
	int way;
	int prefetched;
	int flush_pending;

	enum cgm_cache_block_state_t state;
	enum cgm_cache_block_state_t transient_state;

	//each block has it's own directory (unsigned char)
	union directory_t directory_entry;
	int data_type;

	//for error checking
	long long transient_access_id;
};*/

/*struct cache_set_t{

	int id;

	struct cache_block_t *way_head;
	struct cache_block_t *way_tail;
	struct cache_block_t *blocks;

};*/

/*struct cache_t{

	//star >> my added elements.
	char *name;
	int id;

	enum cache_type_enum cache_type;

	//this is so the cache can advance itself
	eventcount *ec_ptr;

	//cache configuration settings
	unsigned int num_slices;
	unsigned int num_sets;
	unsigned int block_size;
	unsigned int assoc;
	unsigned int num_ports;
	enum cache_policy_t policy;
	char * policy_type;
	int slice_type;
	int bus_width;

	//cache data
	struct cache_set_t *sets;
	unsigned int block_mask;
	int log_block_size;
	unsigned int set_mask;
	int log_set_size;

	//mshr control links
	int mshr_size;
	struct mshr_t *mshrs;

	//outstanding request table
	int **ort;
	struct list_t *ort_list;
	int max_coal;

	//cache queues
	//star todo rewrite all of this queues should be inboxes
	//buffers are internal buffers
	struct list_t *Rx_queue_top;
	struct list_t *Rx_queue_bottom;
	struct list_t *Tx_queue_top;
	struct list_t *Tx_queue_bottom;
	struct list_t *Coherance_Tx_queue;
	struct list_t *Coherance_Rx_queue;
	struct list_t *retry_queue;
	struct list_t *write_back_buffer;
	struct list_t *pending_request_buffer;
	struct list_t *next_queue;
	struct list_t *last_queue;

	//io ctrl
	eventcount volatile *cache_io_up_ec;
	task *cache_io_up_tasks;

	eventcount volatile *cache_io_down_ec;
	task *cache_io_down_tasks;

	//physical characteristics
	unsigned int latency;
	unsigned int wire_latency;
	unsigned int directory_latency;

	//directory bit vectors for coherence
	unsigned int dir_latency;
	union directory_t **dir;
	unsigned int share_mask;

	//L1 I cache protocol virtual functions
	void (*l1_i_fetch)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l1_i_write_block)(struct cache_t *cache, struct cgm_packet_t *message_packet);

	//L1 D cache protocol virtual functions
	void (*l1_d_load)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l1_d_store)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	int (*l1_d_write_block)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l1_d_downgrade)(struct cache_t *cache, struct cgm_packet_t *message_packet);

	//L2 cache protocol virtual functions
	void (*l2_gets)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l2_get)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l2_downgrade_ack)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l2_get_fwd)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	int (*l2_write_block)(struct cache_t *cache, struct cgm_packet_t *message_packet);

	//L3 cache protocol virtual functions
	void (*l3_gets)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l3_get)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l3_downgrade_ack)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l3_downgrade_nack)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*l3_write_block)(struct cache_t *cache, struct cgm_packet_t *message_packet);

	//GPU S cache protocol virtual functions
	void (*gpu_s_load)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*gpu_s_put)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*gpu_s_retry)(struct cache_t *cache, struct cgm_packet_t *message_packet);

	//GPU V cache protocol virtual functions
	void (*gpu_v_load)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*gpu_v_store)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*gpu_v_put)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*gpu_v_retry)(struct cache_t *cache, struct cgm_packet_t *message_packet);

	//GPU L2 cache protocol virtual functions
	void (*gpu_l2_get)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*gpu_l2_put)(struct cache_t *cache, struct cgm_packet_t *message_packet);
	void (*gpu_l2_retry)(struct cache_t *cache, struct cgm_packet_t *message_packet);


	//statistics
	long long fetches;
	long long loads;
	long long stores;
	long long hits;
	long long invalid_hits;
	long long misses;
	long long upgrade_misses;
	long long retries;
	long long coalesces;
	long long mshr_entires;
	long long stalls;
	unsigned int *fetch_address_history;
	unsigned int *load_address_history;
	unsigned int *store_address_history;
};*/
	
	/*#include <cgm/cgm.h>*/
	
	/*printf("L2 id %d get fwd received from L2 id %d cycle %llu\n", l2_caches[my_pid].id, message_packet->l2_cache_id, P_TIME);*/

				/*we have received a get_fwd from the home.
				this is for a block that we have in our core
				and can forward to the requesting core.

				The block should be here in the exclusive or modified state
				however it is possible that the block may be in the wb buffer or
				have been dropped or written back earlier (dirty).

				3 way hop

				if the block is present in cache or wb buffer and exclusive in L2/L1
					(1) downgrade L1 to shared (upper level cache probe).
						(a) move get_fwd to pending request buffer in L2
						(b) send downgrade message to L1 from L2
						(c) check L1 cache and wb buffer for block status
						(d) respond with downgrade ack to L2 from L1
					(2) downgrade L2 to shared.
						(a) receive the downgrade_ack from L1
						(b) pull pending request from buffer
						(c) check L1's inputs from downgrade_ack
						(d) downgrade block to shared
					(3) fwd block to requesting core.
					(4) send downgrade_ack to L3 (home node).
					(5) Done

				if the block is not present in L2/L1
					(1) send downgrade_nack (original GET) to L3
					(2) reply to GET from L3

				//star todo visit this part when working GETX in
				if the block is present and modified (stored) in either L1 or L2
					(1) downgrade L1 to shared and write back (if modified)
					(2) merge and downgrade L2 to shared
					(3) fwd block to requesting core (shared).
					(4) issue sharing WB to L3

				it is possible for the GET_FWD to miss,
				this means the block was silently dropped by the owning node
					(1) send nack to L3 (home)
					(2) L3 sends reply to requester for GET

				L3 locks the block on transactions, so the reply back to L3
				should be a hit. todo check this last statement for correctness*/

				/*star todo adjust for GETX when working the modified states in.*/

				//get the status of the cache block and try to find it in either the cache or wb buffer
				cache_get_block_status(&(l2_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

				/*printf("L2 id %d block hit %d as %s\n", l2_caches[my_pid].id, *cache_block_hit_ptr, str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));*/

				//if hit block is in the L2 and L1 caches
				if(*cache_block_hit_ptr == 1)
				{
					//a GET_FWD means the block is exclusive in this core, but could also be modified
					assert(*cache_block_state_ptr == cgm_cache_block_exclusive || *cache_block_state_ptr == cgm_cache_block_modified);

					//store the get_fwd in the pending request buffer
					message_packet->downgrade_pending = 1;
					cgm_cache_insert_pending_request_buffer(&(l2_caches[my_pid]), message_packet);

					//set the flush_pending bit to 1 in the block
					cgm_cache_set_block_flush_pending_bit(&(l2_caches[my_pid]), message_packet->set, message_packet->way, 1);

					//flush the L1 cache because the line may be dirty in L1
					downgrade_packet = packet_create();
					init_downgrade_packet(downgrade_packet, message_packet->address);

					//send the L1 D cache the downgrade message
					downgrade_packet->cpu_access_type = cgm_access_load;
					list_enqueue(l2_caches[my_pid].Tx_queue_top, downgrade_packet);
					advance(l2_caches[my_pid].cache_io_up_ec);

					/*printf("L2 id %d sends L1 D cache flush\n", l2_caches[my_pid].id);*/
				}
				/*if it is a miss in the cache check the WB buffer for the block*/
				else if(*cache_block_hit_ptr == 0)
				{
					/*//check the WB buffer for the block
					wb_packet = cache_search_wb(&(l2_caches[my_pid]), message_packet->tag, message_packet->set);

					//found the block in the wb buffer
					if(wb_packet)
					{
						//a GET_FWD means the block is exclusive in this core, but could also be modified
						assert(*cache_block_state_ptr == cgm_cache_block_exclusive || *cache_block_state_ptr == cgm_cache_block_modified);

						if(wb_packet->flush_pending == 0)
						{
							//store the get_fwd in the pending request buffer
							wb_packet->downgrade_pending = 1;
							cgm_cache_insert_pending_request_buffer(&(l2_caches[my_pid]), message_packet);
							message_packet = list_remove(l2_caches[my_pid].last_queue, message_packet);
							list_enqueue(l2_caches[my_pid].pending_request_buffer, message_packet);

							//flush the L1 cache because the line may be dirty in L1
							downgrade_packet = packet_create();
							init_downgrade_packet(downgrade_packet, message_packet->address);

							//send the L1 D cache the downgrade message
							downgrade_packet->cpu_access_type = cgm_access_load;
							list_enqueue(l2_caches[my_pid].Tx_queue_top, message_packet);
							advance(l2_caches[my_pid].cache_io_up_ec);

							printf("L2 id %d sends L1 D cache flush\n", l2_caches[my_pid].id);
						}
						//check wb packet state f
						else if(wb_packet->flush_pending == 1)
						{
							flush has already been sent to the L1 D cache.
							Wait for the flush to return.
							fatal("l2 get fwd block in wb with flush pending\n");

						}
					}
					//block isn't in the cache or WB send downgrade_nack to L3
					else
					{*/
						//set downgrade_nack
						message_packet->access_type = cgm_access_downgrade_nack;

						//fwd reply (downgrade_nack) to L3
						l3_map = cgm_l3_cache_map(message_packet->set);

						/*here send the nack down to the L3
						don't change any of the source information*/

						/*message_packet->l2_cache_id = l2_caches[my_pid].id;
						message_packet->l2_cache_name = str_map_value(&l2_strn_map, l2_caches[my_pid].id);
						reply_packet->src_name = l2_caches[my_pid].name;
						reply_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);*/


						message_packet->dest_name = l3_caches[l3_map].name;
						message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

						//charge delay
						P_PAUSE(l2_caches[my_pid].latency);

						//transmit block to requesting node
						cache_put_io_down_queue(&(l2_caches[my_pid]), message_packet);
					/*}*/
				}

void cgm_mesi_l3_get(struct cache_t *cache, struct cgm_packet_t *message_packet){

	enum cgm_access_kind_t access_type;
	/*long long access_id = 0;*/
	int cache_block_hit;
	int cache_block_state;
	int *cache_block_hit_ptr = &cache_block_hit;
	int *cache_block_state_ptr = &cache_block_state;

	access_type = message_packet->access_type;
	/*access_id = message_packet->access_id;*/

	//get the status of the cache block
	cache_get_block_status(&(l3_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

	//get the directory state
	//check the directory dirty bit status
	dirty = cgm_cache_get_dir_dirty_bit(&(l3_caches[my_pid]), message_packet->set, message_packet->way);
	//get number of sharers
	sharers = cgm_cache_get_num_shares(&(l3_caches[my_pid]), message_packet->set, message_packet->way);
	//check to see if access is from an already owning core
	owning_core = cgm_cache_is_owning_core(&(l3_caches[my_pid]), message_packet->set, message_packet->way, message_packet->l2_cache_id);

	switch(*cache_block_state_ptr)
	{
		case cgm_cache_block_noncoherent:
		case cgm_cache_block_owned:
		fatal("l3_cache_ctrl(): Get invalid block state on hit as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
			break;

		case cgm_cache_block_invalid:

			/*printf("l3 load miss\n");*/

			//stats;
			l3_caches[my_pid].misses++;
			assert(message_packet->cpu_access_type == cgm_access_load);

			//check ORT for coalesce
			cache_check_ORT(&(l3_caches[my_pid]), message_packet);

			if(message_packet->coalesced == 1)
				continue;

			//find victim again because LRU has been updated on hits.
			message_packet->l3_victim_way = cgm_cache_replace_block(&(l3_caches[my_pid]), message_packet->set);

			//changes start here
			cgm_L3_cache_evict_block(&(l3_caches[my_pid]), message_packet->set, message_packet->l3_victim_way, sharers);

			//clear the directory entry
			cgm_cache_clear_dir(&(l3_caches[my_pid]), message_packet->set, message_packet->l3_victim_way);

			//add some routing/status data to the packet
			message_packet->access_type = cgm_access_mc_get;

			//star todo this should be exclusive when Get is fully working
			message_packet->cache_block_state = cgm_cache_block_exclusive;
			/*message_packet->cache_block_state = cgm_cache_block_shared;*/

			message_packet->src_name = l3_caches[my_pid].name;
			message_packet->src_id = str_map_string(&node_strn_map, l3_caches[my_pid].name);
			message_packet->dest_id = str_map_string(&node_strn_map, "sys_agent");
			message_packet->dest_name = str_map_value(&node_strn_map, message_packet->dest_id);

			//charge delay
			P_PAUSE(l3_caches[my_pid].latency);

			//transmit to SA/MC
			cache_put_io_down_queue(&(l3_caches[my_pid]), message_packet);
			break;

		case cgm_cache_block_shared:

			//stats;
			l3_caches[my_pid].hits++;

			assert(dirty == 0);

			//update message status
			message_packet->access_type = cgm_access_puts;

			//get the cache block state
			message_packet->cache_block_state = *cache_block_state_ptr;

			//testing
			/*uncomment this when exclusive/modified is working.
			currently the sim by passes the exclusive stage.*/
			assert(*cache_block_state_ptr == cgm_cache_block_shared);
			//there should be at least 1 sharer (after a downgrade)
			/*assert(sharers >= 1);*/

			//set the presence bit in the directory for the requesting core.
			cgm_cache_set_dir(&(l3_caches[my_pid]), message_packet->set, message_packet->way, message_packet->l2_cache_id);

			//set message package size
			message_packet->size = l2_caches[str_map_string(&node_strn_map, message_packet->l2_cache_name)].block_size;

			//update routing
			message_packet->dest_id = str_map_string(&node_strn_map, message_packet->l2_cache_name);
			message_packet->dest_name = str_map_value(&l2_strn_map, message_packet->dest_id);
			message_packet->src_name = l3_caches[my_pid].name;
			message_packet->src_id = str_map_string(&node_strn_map, l3_caches[my_pid].name);

			P_PAUSE(l3_caches[my_pid].latency);

			cache_put_io_up_queue(&(l3_caches[my_pid]), message_packet);

			//check if the packet has coalesced accesses.
			if(access_type == cgm_access_load_retry || message_packet->coalesced == 1)
			{
				//enter retry state.
				cache_coalesed_retry(&(l3_caches[my_pid]), message_packet->tag, message_packet->set);
			}

			break;

		case cgm_cache_block_exclusive:

			//stats;
			l3_caches[my_pid].hits++;

			//star todo update this message when working in GETX
			/*on the first GET the block should have been brought in as exclusive.
			Then it will be a hit on retry with no presence bits set (exclusive).
			On a subsequent access (by either the requesting core or a different core) the block will be here as exclusive,
			if the request comes from the original core the block can be sent as exclusive again.
			if the request comes from a different core the block will need to be downgraded to shared before sending to requesting core.
			Once the block is downgraded to shared it will be in both cores and L3 as shared*/

			assert(sharers >= 0 && sharers <= num_cores);
			assert(owning_core >= 0 && owning_core <= 1);

			//if it is a new access (L3 retry) or a repeat access from an already owning core.
			if(sharers == 0 || owning_core == 1)
			{
				//update message status
				message_packet->access_type = cgm_access_put_clnx;

				//get the cache block state
				message_packet->cache_block_state = *cache_block_state_ptr;

				//testing
				assert(dirty == 0);
				assert(*cache_block_state_ptr == cgm_cache_block_exclusive);

				//set the presence bit in the directory for the requesting core.
				cgm_cache_set_dir(&(l3_caches[my_pid]), message_packet->set, message_packet->way, message_packet->l2_cache_id);

				//set message package size
				message_packet->size = l2_caches[str_map_string(&node_strn_map, message_packet->l2_cache_name)].block_size;

				//update routing headers
				message_packet->dest_id = str_map_string(&node_strn_map, message_packet->l2_cache_name);
				message_packet->dest_name = str_map_value(&l2_strn_map, message_packet->dest_id);
				message_packet->src_name = l3_caches[my_pid].name;
				message_packet->src_id = str_map_string(&node_strn_map, l3_caches[my_pid].name);

				P_PAUSE(l3_caches[my_pid].latency);

				//send the cache block out
				cache_put_io_up_queue(&(l3_caches[my_pid]), message_packet);

				//check if the packet has coalesced accesses.
				if(access_type == cgm_access_load_retry || message_packet->coalesced == 1)
				{
					//enter retry state.
					cache_coalesed_retry(&(l3_caches[my_pid]), message_packet->tag, message_packet->set);
				}
			}
			/*if it is a new access from another core(s).
			We need to downgrade the owning core.
			also, the owning core may have the block dirty
			so we may need to process a sharing writeback*/
			else if (sharers >= 1)
			{
				//testing
				// in the exclusive state there should only be one core with the cache block
				//there better be only one owning core at this stage.
				assert(sharers == 1);

				//delete later
				/*printf("L3 id %d Get fwd sent access id %llu cycle %llu\n", l3_caches[my_pid].id, message_packet->access_id, P_TIME);
				temp_id = message_packet->access_id;*/
				//delete later

				//forward the GET to the owning core*/

				//change the access type
				message_packet->access_type = cgm_access_get_fwd;

				//don't set the block state (yet)

				//don't set the presence bit in the directory for the requesting core (yet).

				//don't change the message package size (yet).

				//set the directory pending bit.
				cgm_cache_set_dir_pending_bit(&(l3_caches[my_pid]), message_packet->set, message_packet->way);

				/*update the routing headers.
				set src as requesting cache and dest as owning cache.
				We can derive the home (directory) later from the original access address.*/

				//get the id of the owning core L2
				owning_core = cgm_cache_get_xown_core(&(l3_caches[my_pid]), message_packet->set, message_packet->way);
				assert(owning_core >= 0 && owning_core < num_cores);

				//owning node
				message_packet->dest_name = str_map_value(&l2_strn_map, owning_core);
				message_packet->dest_id = str_map_string(&node_strn_map, message_packet->dest_name);

				//requesting node L2
				message_packet->src_id = str_map_string(&node_strn_map, message_packet->l2_cache_name);
				message_packet->src_name = str_map_value(&node_strn_map, message_packet->src_id);

				P_PAUSE(l3_caches[my_pid].latency);

				cache_put_io_up_queue(&(l3_caches[my_pid]), message_packet);

				//check if the packet has coalesced accesses.
				if(access_type == cgm_access_load_retry || message_packet->coalesced == 1)
				{
					//enter retry state.
					cache_coalesed_retry(&(l3_caches[my_pid]), message_packet->tag, message_packet->set);
				}

			}

			break;

		case cgm_cache_block_modified:


			if(*cache_block_state_ptr == cgm_cache_block_modified)
			{
				message_packet->access_type = cgm_access_putx;
			}

			fatal("L3 modified cache block without GetX\n");
			break;

	}






}

	
	if(*cache_block_hit_ptr == 1)
				{
					/*//pull the GET_FWD from the pending request buffer
					pending_request = cache_search_pending_request_buffer(&(l2_caches[my_pid]), message_packet->address);

					if not found uh-oh...
					assert(pending_request);
					the address better be the same too...
					assert(pending_request->address == message_packet->address);*/

					//downgrade the local block
					cgm_cache_set_block(&(l2_caches[my_pid]), message_packet->set, message_packet->way, message_packet->tag, cgm_cache_block_shared);

					printf("L2 id %d downgraded to shared\n", l1_d_caches[my_pid].id);

					//prepare to forward the block
					//set access type
					message_packet->access_type = cgm_access_puts;

					//set the block state
					message_packet->cache_block_state = cgm_cache_block_shared;

					//set message package size
					message_packet->size = l2_caches[str_map_string(&node_strn_map, message_packet->l2_cache_name)].block_size;

					//fwd block to requesting core
					//update routing headers swap dest and src
					//requesting node
					message_packet->dest_name = str_map_value(&node_strn_map, message_packet->src_id);
					message_packet->dest_id = str_map_string(&node_strn_map, message_packet->src_name);

					//owning node L2
					message_packet->src_name = l2_caches[my_pid].name;
					message_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);

					printf("requester name %s and id %d\n", message_packet->src_name, message_packet->l2_cache_id);

					temp_id = message_packet->access_id;

					//transmit block to requesting node
					cache_put_io_down_queue(&(l2_caches[my_pid]), message_packet);

					printf("L2 id %d shared block forwarded to L2 cache id %d\n", l1_d_caches[my_pid].id, message_packet->l2_cache_id);
				}
				else
				{
					fatal("uh-oh 2\n");
				}


				///////////////
				//downgrade_ack
				///////////////

				//send the downgrade ack to L3 cache.

				//create downgrade_ack
				reply_packet = packet_create();
				assert(reply_packet);

				init_downgrade_ack_packet(reply_packet, message_packet->address);

				//set size
				reply_packet->size = 1;

				//set requesting core

				//fwd reply (downgrade_ack) to L3
				l3_map = cgm_l3_cache_map(message_packet->set);

				//fakes src as the requester
				/*reply_packet->l2_cache_id = l2_caches[my_pid].id;*/
				reply_packet->l2_cache_id = message_packet->l2_cache_id;
				reply_packet->l2_cache_name = message_packet->src_name;

				reply_packet->src_name = l2_caches[my_pid].name;
				reply_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);
				reply_packet->dest_name = l3_caches[l3_map].name;
				reply_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

				//transmit downgrad_ack to L3 (home)
				list_enqueue(l2_caches[my_pid].Tx_queue_bottom, reply_packet);
				advance(l2_caches[my_pid].cache_io_down_ec);

				/*//destroy the downgrade message because we don't need it anymore.
				message_packet = list_remove(l2_caches[my_pid].last_queue, message_packet);
				free(message_packet);*/

				printf("L2 id %d sent downgrade_ack to L3 id %d cycle %llu\n", l2_caches[my_pid].id, l3_map, P_TIME);

				continue;



/*======================================================================*/
	
	/*void cpu_l1_cache_access_load(struct cache_t *cache, struct cgm_packet_t *message_packet);
void cpu_l1_cache_access_store(struct cache_t *cache, struct cgm_packet_t *message_packet);
void cpu_cache_access_get(struct cache_t *cache, struct cgm_packet_t *message_packet);
void cpu_cache_access_put(struct cache_t *cache, struct cgm_packet_t *message_packet);
void cpu_cache_access_retry(struct cache_t *cache, struct cgm_packet_t *message_packet);*/
	
	/*printf("s cache running access_id %llu cycle %llu list size %d", message_packet->access_id,  P_TIME, list_count(gpu_s_caches[my_pid].last_queue));*/
	
	/*if(l2_caches[i].policy_type == 1)
		{
			l2_caches[i].policy = cache_policy_lru;
		}
		else
		{
			fatal("Invalid cache policy\n");
		}*/
	
	/*gpu_cache_access_retry(&(gpu_s_caches[my_pid]), message_packet);*/
	
	/*gpu_cache_access_put(&(gpu_s_caches[my_pid]), message_packet);*/
	
	//gpu_l1_cache_access_load(&(gpu_s_caches[my_pid]), message_packet);
				/*gpu_l1_cache_access_load(&(gpu_s_caches[my_pid]), message_packet);*/
	
	/*gpu_l1_cache_access_load(&(gpu_v_caches[my_pid]), message_packet);*/
				//gpu_cache_access_load(&(gpu_v_caches[my_pid]), message_packet);
	
	//gpu_cache_access_store(&(gpu_v_caches[my_pid]), message_packet);
				/*gpu_l1_cache_access_store(&(gpu_v_caches[my_pid]), message_packet);*/
	
	/*gpu_cache_access_retry(&(gpu_v_caches[my_pid]), message_packet);*/
	
	/*gpu_cache_access_put(&(gpu_v_caches[my_pid]), message_packet);*/
	
	/*gpu_cache_access_put(&gpu_l2_caches[my_pid], message_packet);*/
				//gpu_l2_cache_access_puts(&gpu_l2_caches[my_pid], message_packet);
	
	/*gpu_cache_access_retry(&gpu_l2_caches[my_pid], message_packet);*/
				//gpu_l2_cache_access_retry(&gpu_l2_caches[my_pid], message_packet);
	
	
	/*gpu_cache_access_get(&gpu_l2_caches[my_pid], message_packet);*/
				//gpu_l2_cache_access_gets(&gpu_l2_caches[my_pid], message_packet);
				
				
	
	/*void cpu_l1_cache_access_load(struct cache_t *cache, struct cgm_packet_t *message_packet){

	struct cgm_packet_t *ort_packet;
	//struct cgm_packet_t *miss_status_packet;
	enum cgm_access_kind_t access_type;
	unsigned int addr = 0;
	long long access_id = 0;

	int set = 0;
	int tag = 0;
	unsigned int offset = 0;
	int way = 0;
	int state = 0;

	int *set_ptr = &set;
	int *tag_ptr = &tag;
	unsigned int *offset_ptr = &offset;
	int *way_ptr = &way;
	int *state_ptr = &state;

	int cache_status = 0;
	int mshr_status = -1;
	int mshr_row = -1;

	int i = 0;
	int row = 0;

	//stats
	cache->loads++;

	//access information
	access_type = message_packet->access_type;
	access_id = message_packet->access_id;
	addr = message_packet->address;

	//probe the address for set, tag, and offset.
	cgm_cache_probe_address(cache, addr, set_ptr, tag_ptr, offset_ptr);

	//store the decode
	message_packet->tag = tag;
	message_packet->set = set;
	message_packet->offset = offset;

	CGM_DEBUG(CPU_cache_debug_file,"%s access_id %llu cycle %llu as %s addr 0x%08u, tag %d, set %d, offset %u\n",
			cache->name, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, access_type), addr, *tag_ptr, *set_ptr, *offset_ptr);


	//get the block and the state of the block and charge cycles

	//////testing
	if(l1_i_inf && cache->cache_type == l1_i_cache_t)
	{
		cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
		cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cgm_cache_block_shared);
	}

	if(l1_d_inf && cache->cache_type == l1_d_cache_t)
	{
		cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
		cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cgm_cache_block_shared);

	}

	if(l1_i_miss && cache->cache_type == l1_i_cache_t)
	{
		cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
		cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cgm_cache_block_invalid);
	}

	if(l1_d_miss && cache->cache_type == l1_d_cache_t)
	{
		cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
		cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cgm_cache_block_invalid);
	}
	//////testing

	//get the block and the state of the block and charge cycles
	cache_status = cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

	//update way list for LRU if block is present.
	if(cache_status == 1)
	{
		cgm_cache_access_block(cache, set, way);
	}

	//Cache Hit!
	if(cache_status == 1 && *state_ptr != 0)
	{
		assert(*state_ptr != cgm_cache_block_invalid);
		CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu hit state %s\n", cache->name, access_id, P_TIME, str_map_value(&cgm_cache_block_state_map, *state_ptr));

		if(*state_ptr == cgm_cache_block_modified || *state_ptr == cgm_cache_block_exclusive || *state_ptr == cgm_cache_block_shared || *state_ptr == cgm_cache_block_noncoherent)
		{
			cache->hits++;

			//CPU L1 I cache
			if(message_packet->access_type == cgm_access_fetch)
			{
				//should only ever be shared for i caches.
				assert(*state_ptr != cgm_cache_block_modified || *state_ptr != cgm_cache_block_exclusive || *state_ptr != cgm_cache_block_noncoherent);

				//remove packet from cache queue, global queue, and simulator memory
				message_packet = list_remove(cache->last_queue, message_packet);

				remove_from_global(access_id);

				packet_destroy(message_packet);

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu cleared from mem system\n", cache->name, access_id, P_TIME);
			}
			//CPU L1 D cache
			if(message_packet->access_type == cgm_access_load)
			{
				message_packet = list_remove(cache->last_queue, message_packet);

				linked_list_add(message_packet->event_queue, message_packet->data);

				packet_destroy(message_packet);
			}
		}
		else
		{
			fatal("cpu_l1_cache_access_load(): incorrect block state set");
		}

	}
	//Cache miss or cache block is invalid
	else if(cache_status == 0 || *state_ptr == 0)
	{
		cache->misses++;

		CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu miss\n", cache->name, access_id, P_TIME);

		//CPU L1 I cache
		if(message_packet->access_type == cgm_access_fetch)
		{
			message_packet->cpu_access_type = cgm_access_fetch;
			message_packet->access_type = cgm_access_gets_i;
			message_packet->l1_access_type = cgm_access_gets_i;
		}
		//CPU L1 D cache
		else if(message_packet->access_type == cgm_access_load)
		{
			message_packet->cpu_access_type = cgm_access_load;
			message_packet->access_type = cgm_access_get;
			message_packet->l1_access_type = cgm_access_get;
		}
		else
		{
			fatal("cpu_l1_cache_access_load(): invalid CPU l1 cache access type access_id %llu cycle %llu", access_id, P_TIME);
		}

		//miss so check ORT status
		i = ort_search(cache, tag, set);

		//entry was not found
		if(i == cache->mshr_size)
		{
			//get an empty row and set the ORT values.
			row = get_ort_status(cache);
			assert(row < cache->mshr_size);
			ort_set(cache, row, tag, set);

			//forward message_packet
			P_PAUSE(cache->latency);

			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu l2 queue free size %d\n",
				cache->name, access_id, P_TIME, list_count(l2_caches[cache->id].Rx_queue_top));

			change the access type for the coherence protocol and drop into the L2's queue
			remove the access from the l1 cache queue and place it in the l2 cache ctrl queue
			list_remove(cache->last_queue, message_packet);
			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
					cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

			//star here
			list_enqueue(cache->Tx_queue_bottom, message_packet);
			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu l2_cache[%d] as %s\n",
					cache->name, access_id, P_TIME, cache->id, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));
			CGM_DEBUG(protocol_debug_file, "Access_id %llu cycle %llu %s Miss SEND %s %s\n",
					access_id, P_TIME, cache->name, l2_caches[cache->id].name, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

			//advance the L2 cache adding some wire delay time.
			//future_advance(&l2_cache[cache->id], WIRE_DELAY(l2_caches[cache->id].wire_latency));

			//list_enqueue(l2_caches[cache->id].Rx_queue_top, message_packet);
			//advance(&l2_cache[cache->id]);
			advance(cache->cache_io_down_ec);

		}
		else if (i >= 0 && i < cache->mshr_size)
		{
			//entry found in ORT so coalesce access
			assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);

			P_PAUSE(cache->latency);

			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu coalesced\n",
					cache->name, access_id, P_TIME);

			list_remove(cache->last_queue, message_packet);
			list_enqueue(cache->ort_list, message_packet);
		}
		else
		{
			fatal("cpu_l1_cache_access_load(): %s i outside of bounds\n", cache->name);
		}
	}
	return;
}

void cpu_l1_cache_access_store(struct cache_t *cache, struct cgm_packet_t *message_packet){

	struct cgm_packet_t *miss_status_packet;
	enum cgm_access_kind_t access_type;
	unsigned int addr = 0;
	long long access_id = 0;

	int set = 0;
	int tag = 0;
	unsigned int offset = 0;
	int way = 0;
	int state = 0;

	int *set_ptr = &set;
	int *tag_ptr = &tag;
	unsigned int *offset_ptr = &offset;
	int *way_ptr = &way;
	int *state_ptr = &state;

	int cache_status = 0;
	int mshr_status = 0;

	int i = 0;
	int row = 0;

	//stats
	cache->stores++;

	//access information
	access_type = message_packet->access_type;
	access_id = message_packet->access_id;
	addr = message_packet->address;

	//probe the address for set, tag, and offset.
	cgm_cache_probe_address(cache, addr, set_ptr, tag_ptr, offset_ptr);
	//store the decode for later
	message_packet->tag = tag;
	message_packet->set = set;
	message_packet->offset = offset;

	//////testing

	//get the block and the state of the block and charge cycles
	if(l1_d_inf && cache->cache_type == l1_d_cache_t)
	{
		cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
		cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
	}

	if(l1_d_miss && cache->cache_type == l1_d_cache_t)
	{
		cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
		cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_invalid);
	}
	//////testing

	CGM_DEBUG(CPU_cache_debug_file,"%s access_id %llu cycle %llu as %s addr 0x%08u, tag %d, set %d, offset %u\n",
			cache->name, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, access_type), addr, *tag_ptr, *set_ptr, *offset_ptr);


	//get the block and the state of the block and charge cycles
	cache_status = cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

	//Cache Hit!
	if(cache_status == 1 && *state_ptr != 0)// && *state_ptr != cache_block_shared)
	{
		//check state of the block
		//block is valid

		assert(*state_ptr != cache_block_invalid);

		CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu hit state %s\n", cache->name, access_id, P_TIME, str_map_value(&cache_block_state_map, *state_ptr));

		//star todo this is wrong
		if(*state_ptr == cache_block_modified || *state_ptr == cache_block_exclusive || *state_ptr == cache_block_shared || *state_ptr == cache_block_noncoherent)
		{
			cache->hits++;

			//star todo change this to work as a message sent to the directory
			//also need to send invalidations out.
			if(*state_ptr == cache_block_exclusive || *state_ptr == cache_block_shared)
			{
				cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_modified);
			}

			//here we would write the data into the block if we had the correct access.

			message_packet = list_remove(cache->last_queue, message_packet);

			linked_list_add(message_packet->event_queue, message_packet->data);

			packet_destroy(message_packet);

			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu cleared from mem system\n", cache->name, access_id, P_TIME);
		}
		else
		{
			fatal("cpu_l1_cache_access_store(): incorrect block state set");
		}
	}
	//Cache Miss!
	else if(cache_status == 0 || *state_ptr == 0)
	{
		cache->misses++;

		//on both a miss and invalid hit the state_ptr should be zero
		CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu miss\n", cache->name, access_id, P_TIME);

		//only the D$ stores
		message_packet->cpu_access_type = cgm_access_store;
		message_packet->access_type = cgm_access_get;
		message_packet->l1_access_type = cgm_access_get;

		//miss so check ORT status
		i = ort_search(cache, tag, set);

		//entry was not found
		if(i == cache->mshr_size)
		{
			//get an empty row and set the ORT values.
			row = get_ort_status(cache);
			assert(row < cache->mshr_size);
			ort_set(cache, row, tag, set);

			P_PAUSE(cache->latency);

			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu l2 queue free size %d\n",
				cache->name, access_id, P_TIME, list_count(l2_caches[cache->id].Rx_queue_top));

			change the access type for the coherence protocol and drop into the L2's queue
			remove the access from the l1 cache queue and place it in the l2 cache ctrl queue

			message_packet = list_remove(cache->last_queue, message_packet);
			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
					cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

			//list_enqueue(l2_caches[cache->id].Rx_queue_top, message_packet);
			list_enqueue(cache->Tx_queue_bottom, message_packet);
			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu %s as %s\n",
					cache->name, access_id, P_TIME, l2_caches[cache->id].name, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));
			CGM_DEBUG(protocol_debug_file, "%s Access_id %llu cycle %llu %s miss SEND %s %s\n",
					cache->name, access_id, P_TIME, cache->name, l2_caches[cache->id].name, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

			//advance the L2 cache adding some wire delay time.
			//advance(&l2_cache[cache->id]);
			advance(cache->cache_io_down_ec);
		}
		else if (i >= 0 && i < cache->mshr_size)
		{
			//entry found in ORT so coalesce access
			assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);

			P_PAUSE(cache->latency);

			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu coalesced\n",
					cache->name, access_id, P_TIME);

			list_remove(cache->last_queue, message_packet);
			list_enqueue(cache->ort_list, message_packet);
		}
		else
		{
			fatal("cpu_l1_cache_access_store(): %s i outside of bounds\n", cache->name);
		}
	}
	return;
}

void cpu_cache_access_get(struct cache_t *cache, struct cgm_packet_t *message_packet){

	struct cgm_packet_status_t *miss_status_packet;
	enum cgm_access_kind_t access_type;
	unsigned int addr = 0;
	long long access_id = 0;

	int set = 0;
	int tag = 0;
	unsigned int offset = 0;
	int way = 0;
	int state = 0;

	int *set_ptr = &set;
	int *tag_ptr = &tag;
	unsigned int *offset_ptr = &offset;
	int *way_ptr = &way;
	int *state_ptr = &state;

	int cache_status;
	int mshr_status = 0;
	int l3_map = -1;

	int i = 0;
	int row = 0;

	access_type = message_packet->access_type;
	access_id = message_packet->access_id;
	addr = message_packet->address;

	//stats
	cache->loads++;

	//probe the address for set, tag, and offset.
	cgm_cache_probe_address(cache, addr, set_ptr, tag_ptr, offset_ptr);

	CGM_DEBUG(CPU_cache_debug_file,"%s access_id %llu cycle %llu as %s addr 0x%08u, tag %d, set %d, offset %u\n",
			cache->name, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, access_type), addr, *tag_ptr, *set_ptr, *offset_ptr);


	//////testing
	if(l2_inf && cache->cache_type == l2_cache_t)
	{
		cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
		cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
	}
	else if(l3_inf && cache->cache_type == l3_cache_t)
	{
		cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
		cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
	}
	else if(l2_miss || l3_miss)
	{
		fatal("l2 and l3 caches set to miss");
	}
	//////testing

	//look up, and charge a cycle.
	cache_status = cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

	//Cache Hit!
	if(cache_status == 1 && *state_ptr != 0)
	{

		CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu hit\n", cache->name, access_id, P_TIME);

		assert(*state_ptr != cache_block_invalid);

		if(*state_ptr == cache_block_modified || *state_ptr == cache_block_exclusive || *state_ptr == cache_block_shared || *state_ptr == cache_block_noncoherent)
		{

			cache->hits++;


			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
					cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

			if(cache->cache_type == l2_cache_t)
			{
				//send to correct l1 cache and change access type
				if (message_packet->access_type == cgm_access_gets_i)
				{
					//while the next level of cache's in queue is full stall
					while(!cache_can_access_bottom(&l1_i_caches[cache->id]))
					{
						printf("l2 hit reply stall\n");
						P_PAUSE(1);
					}

					CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu L1 bottom queue free size %d\n",
							cache->name, access_id, P_TIME, list_count(l1_i_caches[cache->id].Rx_queue_bottom));

					P_PAUSE(cache->latency);

					message_packet->access_type = cgm_access_puts;
					message_packet->cache_block_state = *state_ptr;

					message_packet = list_remove(cache->last_queue, message_packet);
					list_enqueue(cache->Tx_queue_top, message_packet);
					advance(cache->cache_io_up_ec);
					//list_enqueue(l1_i_caches[cache->id].Rx_queue_bottom, message_packet);
					//advance(&l1_i_cache[cache->id]);
					//future_advance(&l1_i_cache[cache->id], WIRE_DELAY(l1_i_caches[cache->id].wire_latency));
				}
				else if (message_packet->access_type == cgm_access_get)
				{
					//while the next level of cache's in queue is full stall
					while(!cache_can_access_bottom(&l1_d_caches[cache->id]))
					{
						printf("l2 hit reply stall\n");
						P_PAUSE(1);
					}

					CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu %s free size %d\n",
							cache->name, access_id, P_TIME, l1_d_caches[cache->id].Rx_queue_bottom->name, list_count(l1_d_caches[cache->id].Rx_queue_bottom));

					P_PAUSE(cache->latency);

					message_packet->access_type = cgm_access_puts;
					message_packet->cache_block_state = *state_ptr;

					message_packet = list_remove(cache->last_queue, message_packet);
					list_enqueue(cache->Tx_queue_top, message_packet);
					advance(cache->cache_io_up_ec);
					//list_enqueue(l1_d_caches[cache->id].Rx_queue_bottom, message_packet);
					//advance(&l1_d_cache[cache->id]);
					//future_advance(&l1_d_cache[cache->id], WIRE_DELAY(l1_d_caches[cache->id].wire_latency));
				}
				else
				{
					fatal("l2_cache_access_gets(): %s access_id %llu cycle %llu incorrect access type %s\n",
							cache->name, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));
				}

			}

			else if(cache->cache_type == l3_cache_t)
			{
				//This is a hit in the L3 cache, send up to L2 cache
				//while the next level of cache's in queue is full stall
				while(!switch_can_access(switches[cache->id].south_queue))
				{
					printf("l3 cache up stall\n");
					P_PAUSE(1);
				}

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu switch south queue free size %d\n",
						cache->name, access_id, P_TIME, list_count(switches[cache->id].south_queue));

				P_PAUSE(cache->latency);

				//success
				message_packet->access_type = cgm_access_puts;
				message_packet->cache_block_state = *state_ptr;

				message_packet->dest_name = message_packet->src_name; //strdup(message_packet->src_name);
				message_packet->dest_id = message_packet->src_id;
				message_packet->src_name = cache->name; //strdup(cache->name);
				message_packet->src_id = str_map_string(&node_strn_map, cache->name);

				message_packet = list_remove(cache->last_queue, message_packet);

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
						cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

				//list_enqueue(switches[cache->id].south_queue, message_packet);
				//advance(&switches_ec[cache->id]);
				//future_advance(&switches_ec[cache->id], WIRE_DELAY(switches[cache->id].wire_latency));

				list_enqueue(cache->Tx_queue_top, message_packet);
				advance(cache->cache_io_up_ec);
				//done

			}
			else
			{
				fatal("cpu_cache_access_get(): hit bad cache type access_id %llu cycle %llu\n", access_id, P_TIME);
			}

		}
		else
		{
			fatal("cpu_cache_access_load(): incorrect block state set");
		}

		CGM_DEBUG(protocol_debug_file, "Access_id %llu cycle %llu %s Hit SEND %s to %s\n",
			access_id, P_TIME, cache->name, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), l1_i_caches[cache->id].name);

	}
	//Cache Miss!
	else if(cache_status == 0 || *state_ptr == 0)
	{

		cache->misses++;

		CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu miss\n", cache->name, access_id, P_TIME);

		if(cache->cache_type == l2_cache_t)
		{
			//miss so check ORT status
			i = ort_search(cache, tag, set);

			//entry was not found
			if(i == cache->mshr_size)
			{
				//get an empty row and set the ORT values.
				row = get_ort_status(cache);
				assert(row < cache->mshr_size);
				ort_set(cache, row, tag, set);

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu miss switch north queue free size %d\n",
						cache->name, access_id, P_TIME, list_count(switches[cache->id].north_queue));

				P_PAUSE(cache->latency);

				l3_map = cgm_l3_cache_map(set_ptr);
				message_packet->access_type = cgm_access_gets;
				message_packet->l2_cache_id = cache->id;
				message_packet->l2_cache_name = str_map_value(&l2_strn_map, cache->id);

				message_packet->src_name = cache->name;
				message_packet->src_id = str_map_string(&node_strn_map, cache->name);
				message_packet->dest_name = l3_caches[l3_map].name;
				message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

				message_packet = list_remove(cache->last_queue, message_packet);
				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
						cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

				//list_enqueue(switches[cache->id].north_queue, message_packet);

				list_enqueue(cache->Tx_queue_bottom, message_packet);

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu l3_cache[%d] send %s\n",
						cache->name, access_id, P_TIME, l3_map, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

				CGM_DEBUG(protocol_debug_file, "Access_id %llu cycle %llu %s Miss SEND %s %s\n",
						access_id, P_TIME, cache->name, l3_caches[l3_map].name, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

				//advance the L2 cache adding some wire delay time.
				//advance(&switches_ec[cache->id]);
				//future_advance(&switches_ec[cache->id], WIRE_DELAY(switches[cache->id].wire_latency));

				advance(cache->cache_io_down_ec);

			}
			else if (i >= 0 && i < cache->mshr_size)
			{
				//entry found in ORT so coalesce access
				assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu coalesced\n",
						cache->name, access_id, P_TIME);

				list_remove(cache->last_queue, message_packet);
				list_enqueue(cache->ort_list, message_packet);
			}
			else
			{
				fatal("cpu_l1_cache_access_store(): %s i outside of bounds\n", cache->name);
			}
		}
		else if(cache->cache_type == l3_cache_t)
		{
			//get an empty row and set the ORT values.
			row = get_ort_status(cache);
			assert(row < cache->mshr_size);
			ort_set(cache, row, tag, set);

			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu %s free size %d\n",
				cache->name, access_id, P_TIME, switches[cache->id].south_queue->name, list_count(switches[cache->id].south_queue));

			P_PAUSE(cache->latency);

			message_packet->size = 0;
			message_packet->src_name = cache->name;
			message_packet->src_id = str_map_string(&node_strn_map, cache->name);
			message_packet->dest_id = str_map_string(&node_strn_map, "sys_agent");
			message_packet->dest_name = str_map_value(&node_strn_map, message_packet->dest_id);

			//success
			message_packet = list_remove(cache->last_queue, message_packet);

			//list_enqueue(switches[cache->id].south_queue, message_packet);
			list_enqueue(cache->Tx_queue_bottom, message_packet);

			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu l3_cache[%d] as %s\n",
					cache->name, access_id, P_TIME, cache->id, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

			CGM_DEBUG(protocol_debug_file, "Access_id %llu cycle %llu %s Miss SEND %s %s\n",
				access_id, P_TIME, cache->name, system_agent->name, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

			//advance(&switches_ec[cache->id]);
			advance(cache->cache_io_down_ec);

		}
		else
		{
			fatal("cpu_cache_access_get(): miss bad cache type access_id %llu cycle %llu type %s \n",
					access_id, P_TIME, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));
		}
	}
	return;
}

void cpu_cache_access_put(struct cache_t *cache, struct cgm_packet_t *message_packet){

	struct cgm_packet_t *ort_packet;
	enum cgm_access_kind_t access_type;
	unsigned int addr = 0;
	long long access_id = 0;
	int set = 0;
	int tag = 0;
	unsigned int offset = 0;
	int way = 0;
	int state = 0;

	int *set_ptr = &set;
	int *tag_ptr = &tag;
	unsigned int *offset_ptr = &offset;
	int *way_ptr = &way;
	int *state_ptr = &state;

	int mshr_row = -1;

	int row = 0;
	int adv = 0;

	assert(message_packet != NULL);

	access_type = message_packet->access_type;
	addr = message_packet->address;
	access_id = message_packet->access_id;

	//probe the address for set, tag, and offset.
	//cgm_cache_probe_address(cache, addr, set_ptr, tag_ptr, offset_ptr);

	CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu puts\n", cache->name, access_id, P_TIME);

	//block is returned so find it in the ORT
	//clear the entry from the ort

	//printf("access_id %llu recieved cycle %llu as %s\n", message_packet->access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

	//printf("access_id %llu COAL %d tag %d set %d offset %d l1_way %d cycle %llu\n",message_packet->access_id, message_packet->coalesced, message_packet->tag, message_packet->set, message_packet->offset, message_packet->l1_victim_way, P_TIME);
	//getchar();

	row = ort_search(cache, message_packet->tag, message_packet->set);
	assert(row < cache->mshr_size);
	ort_clear(cache, row);

	//printf("access_id %llu cleared from ORT %d cycle %llu as %s\n", message_packet->access_id, row, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));
	cgm_cache_get_block(cache, set, way, NULL, state_ptr);

	//cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
	printf("CFB way = %d\n", *way_ptr);
	//getchar();

	cgm_cache_set_block(cache, message_packet->set, message_packet->l1_victim_way, message_packet->tag, cache_block_shared);

	//cgm_cache_set_block(cache, message_packet->set, message_packet->l1_victim_way, message_packet->tag, cache_block_shared);

	//find a victim
	if(cache->cache_type == l1_i_cache_t)
	{
		//Evict, get the LRU
		*(way_ptr) = cgm_cache_replace_block(cache, set);

		//get the state of the block
		cgm_cache_get_block(cache, set, way, NULL, state_ptr);

		//i cache blocks should either be in the shared or invalid state (initialized).
		if(*state_ptr == cache_block_invalid || *state_ptr == cache_block_shared)
		{
			//with i cache it is ok to evict and just drop the block
			cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
		}
		else
		{
			fatal("cpu_cache_access_put(): i cache invalid block state\n");
		}

		CGM_DEBUG(CPU_cache_debug_file, " %s put tag %d set %d\n", cache->name, tag, set);
		CGM_DEBUG(CPU_cache_debug_file, "Before\n");
		CGM_DEBUG(CPU_cache_debug_file, "cache->sets[set].blocks[0].tag = tag; %d\n", cache->sets[set].blocks[0].tag);
		CGM_DEBUG(CPU_cache_debug_file, "cache->sets[set].blocks[0].state = state; %d\n", cache->sets[set].blocks[0].state);
		CGM_DEBUG(CPU_cache_debug_file, "cache->sets[set].blocks[1].tag = tag; %d\n", cache->sets[set].blocks[1].tag);
		CGM_DEBUG(CPU_cache_debug_file, "cache->sets[set].blocks[1].state = state; %d\n", cache->sets[set].blocks[1].state);

		CGM_DEBUG(CPU_cache_debug_file, "After\n");
		CGM_DEBUG(CPU_cache_debug_file, "cache->sets[set].blocks[0].tag = tag; %d\n", cache->sets[set].blocks[0].tag);
		CGM_DEBUG(CPU_cache_debug_file, "cache->sets[set].blocks[0].state = state; %d\n", cache->sets[set].blocks[0].state);
		CGM_DEBUG(CPU_cache_debug_file, "cache->sets[set].blocks[1].tag = tag; %d\n", cache->sets[set].blocks[1].tag);
		CGM_DEBUG(CPU_cache_debug_file, "cache->sets[set].blocks[1].state = state; %d\n", cache->sets[set].blocks[1].state);

	}
	else if(cache->cache_type == l1_d_cache_t)
	{

		//Evict, get the LRU
		*(way_ptr) = cgm_cache_replace_block(cache, set);

		//get the state of the block
		cgm_cache_get_block(cache, set, way, NULL, state_ptr);

		//d cache blocks can either be in the shared, exclusive, modified or invalid state (initialized and invalidated).
		//for a single core machine the d cache line will always be exclusive, modified, or invalid.
		if(*state_ptr == cache_block_invalid || *state_ptr == cache_block_shared || *state_ptr == cache_block_exclusive)
		{
			//if the block is invalid it is ok to drop the line.
			cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
		}
		else if (*state_ptr == cache_block_modified)
		{
			//star todo write back is required.
			cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
		}
		else
		{
			fatal("cpu_cache_access_put(): i cache invalid block state\n");
		}

	}
	else if(cache->cache_type == l2_cache_t)
	{
		//Evict, get the LRU
		*(way_ptr) = cgm_cache_replace_block(cache, set);

		//get the state of the block
		cgm_cache_get_block(cache, set, way, NULL, state_ptr);

		if(*state_ptr == cache_block_invalid || *state_ptr == cache_block_shared || *state_ptr == cache_block_exclusive)
		{
			//if the block is invalid it is ok to drop the line.
			cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
		}
		else if (*state_ptr == cache_block_modified)
		{
			//star todo write back is required.
			cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
		}
		else
		{
			fatal("cpu_cache_access_put(): i cache invalid block state\n");
		}

	}
	else if(cache->cache_type == l3_cache_t)
	{
		//Evict, get the LRU
		*(way_ptr) = cgm_cache_replace_block(cache, set);

		//get the state of the block
		cgm_cache_get_block(cache, set, way, NULL, state_ptr);

		if(*state_ptr == cache_block_invalid || *state_ptr == cache_block_shared || *state_ptr == cache_block_exclusive)
		{
			//if the block is invalid it is ok to drop the line.
			cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
		}
		else if (*state_ptr == cache_block_modified)
		{
			//star todo write back is required.
			cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
		}
		else
		{
			fatal("cpu_cache_access_put(): i cache invalid block state\n");
		}
	}

	//set retry

	//printf("access_id %llu COAL %d tag %d set %d offset %d cycle %llu\n", message_packet->access_id, message_packet->coalesced, message_packet->tag, message_packet->set, message_packet->offset, P_TIME);

	message_packet->access_type = cgm_access_retry;

	message_packet = list_remove(cache->last_queue, message_packet);
	list_enqueue(cache->retry_queue, message_packet);
	advance(cache->ec_ptr);

	return;
}

void cpu_cache_access_retry(struct cache_t *cache, struct cgm_packet_t *message_packet){

	enum cgm_access_kind_t access_type;
	unsigned int addr = 0;
	long long access_id = 0;
	int set = 0;
	int tag = 0;
	unsigned int offset = 0;
	int way = 0;
	int state = 0;
	int cache_status;

	int *set_ptr = &set;
	int *tag_ptr = &tag;
	unsigned int *offset_ptr = &offset;
	int *way_ptr = &way;
	int *state_ptr = &state;

	//int i = 0;

	access_type = message_packet->access_type;
	access_id = message_packet->access_id;
	addr = message_packet->address;

	//stats
	cache->retries++;

	//probe the address for set, tag, and offset.
	//cgm_cache_probe_address(cache, addr, set_ptr, tag_ptr, offset_ptr);

	CGM_DEBUG(CPU_cache_debug_file,"%s access_id %llu cycle %llu as %s addr 0x%08u, tag %d, set %d, offset %u\n",
		cache->name, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, access_type), addr, *tag_ptr, *set_ptr, *offset_ptr);

	//look it up.
	*tag_ptr = message_packet->tag;
	*set_ptr = message_packet->set;
	*offset_ptr = message_packet->offset;
	*way_ptr = message_packet->l1_victim_way;
	*way_ptr = 1;

	cache_status = cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

	//printf("access_id %llu retry cache status %d way %d way %d state %d cycle %llu as %s\n",
			//message_packet->access_id, cache_status, *way_ptr, message_packet->l1_victim_way, *state_ptr, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));
	//printf("access_id %llu COAL %d tag %d set %d offset %d cycle %llu\n", message_packet->access_id, message_packet->coalesced, message_packet->tag, message_packet->set, message_packet->offset, P_TIME);

	//Cache Hit!
	if(cache_status == 1 && *state_ptr != 0)
	{
		CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu retry hit\n", cache->name, access_id, P_TIME);

		if(*state_ptr == cache_block_modified || *state_ptr == cache_block_exclusive || *state_ptr == cache_block_shared || *state_ptr == cache_block_noncoherent)
		{

			//list_remove(cache->retry_queue, message_packet); /*check here
			CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
				cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

			if(cache->cache_type == l1_i_cache_t || cache->cache_type == l1_d_cache_t)
			{
				//CPU L1 I cache
				if(message_packet->cpu_access_type == cgm_access_fetch)
				{
					//clear out the returned memory request
					//remove packet from cache retry queue and global queue
					P_PAUSE(cache->latency);

					//printf("access_id %llu cleared cycle %llu as %s\n",
							//message_packet->access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

					message_packet = list_remove(cache->last_queue, message_packet); check here
					free(message_packet);
					remove_from_global(access_id);

				}
				//CPU L1 D cache
				else if(message_packet->cpu_access_type == cgm_access_load || message_packet->cpu_access_type == cgm_access_store)
				{
					P_PAUSE(cache->latency);

					message_packet = list_remove(cache->last_queue, message_packet); check here
					linked_list_add(message_packet->event_queue, message_packet->data);
					free(message_packet);

					//retry coalesced packets.
					cpu_cache_coalesced_retry(cache, tag_ptr, set_ptr);
				}

			}
			else if(cache->cache_type == l2_cache_t)
			{

				if (message_packet->l1_access_type == cgm_access_gets_i)
				{
					//while the next level of cache's in queue is full stall
					while(!cache_can_access_bottom(&l1_i_caches[cache->id]))
					{
						//printf("stall\n");
						//getchar();
						P_PAUSE(1);
					}

						CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu L1 bottom queue free size %d\n",
								cache->name, access_id, P_TIME, list_count(l1_i_caches[cache->id].Rx_queue_bottom));

						P_PAUSE(cache->latency);

						message_packet->access_type = cgm_access_puts;

						message_packet = list_remove(cache->last_queue, message_packet); check here

						list_enqueue(cache->Tx_queue_top, message_packet);
						advance(cache->cache_io_up_ec);

						//list_enqueue(l1_i_caches[cache->id].Rx_queue_bottom, message_packet);
						//advance(&l1_i_cache[cache->id]);
						//future_advance(&l1_i_cache[cache->id], WIRE_DELAY(l1_i_caches[cache->id].wire_latency));

						//retry coalesced packets.
						cpu_cache_coalesced_retry(cache, tag_ptr, set_ptr);

				}
				else if (message_packet->l1_access_type == cgm_access_get)
				{
					//while the next level of cache's in queue is full stall
					while(!cache_can_access_bottom(&l1_d_caches[cache->id]))
					{
						//printf("stall\n");
						//getchar();
						P_PAUSE(1);
					}

					P_PAUSE(cache->latency);

					CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu %s free size %d\n",
							cache->name, access_id, P_TIME, l1_d_caches[cache->id].Rx_queue_bottom->name, list_count(l1_d_caches[cache->id].Rx_queue_bottom));

					message_packet->access_type = cgm_access_puts;

					message_packet = list_remove(cache->last_queue, message_packet); check here
					//list_enqueue(l1_d_caches[cache->id].Rx_queue_bottom, message_packet);
					//advance(&l1_d_cache[cache->id]);
					//future_advance(&l1_d_cache[cache->id], WIRE_DELAY(l1_d_caches[cache->id].wire_latency));

					list_enqueue(cache->Tx_queue_top, message_packet);
					advance(cache->cache_io_up_ec);

					//retry coalesced packets.
					cpu_cache_coalesced_retry(cache, tag_ptr, set_ptr);
				}
				else
				{
					fatal("l2_cache_access_gets(): %s access_id %llu cycle %llu incorrect l1 access type %s\n",
							cache->name, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->l1_access_type));
				}

			}
			else if(cache->cache_type == l3_cache_t)
			{
				//Send up to L2 cache
				while(!switch_can_access(switches[cache->id].south_queue))
				{
					P_PAUSE(1);
				}

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu switch south queue free size %d\n",
						cache->name, access_id, P_TIME, list_count(switches[cache->id].south_queue));

				P_PAUSE(cache->latency);

				//success
				//remove packet from l3 cache in queue
				assert(message_packet != NULL);

				message_packet->access_type = cgm_access_puts;


				int l2_map = cgm_l2_cache_map(message_packet->l2_cache_id);
				message_packet->dest_id = str_map_string(&node_strn_map, message_packet->l2_cache_name);
				message_packet->dest_name = str_map_value(&l2_strn_map, message_packet->dest_id);
				message_packet->src_name = cache->name;
				message_packet->src_id = str_map_string(&node_strn_map, cache->name);




				list_remove(cache->last_queue, message_packet);
				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
						cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

				list_enqueue(switches[cache->id].south_queue, message_packet);

				advance(&switches_ec[cache->id]);
			}
			else
			{

				fatal("problem\n");
			}

			//retry coalesced packets.
			cpu_cache_coalesced_retry(cache, tag_ptr, set_ptr);

		}
		else
		{
			fatal("cpu_cache_access_retry(): incorrect block state set");
		}
	}
	else
	{
		fatal("cpu_cache_access_retry(): %s access_id %llu retry miss\n", cache->name, access_id);
	}
	return;
}
	
	
====================================================================	
	/*gpu_l2_caches[i].mshrs = (void *) calloc(gpu_l2_caches[i].mshr_size, sizeof(struct mshr_t));
		for(j = 0; j < gpu_l2_caches[i].mshr_size; j++)
		{
			memset (buff,'\0' , 100);
			snprintf(buff, 100, "gpu_l2_caches[%d].mshr[%d]", i, j);
			gpu_l2_caches[i].mshrs[j].name = strdup(buff);

			gpu_l2_caches[i].mshrs[j].entires = list_create();
			memset (buff,'\0' , 100);
			snprintf(buff, 100, "gpu_l2_caches[%d].mshr[%d].entires", i, j);
			gpu_l2_caches[i].mshrs[j].entires->name = strdup(buff);

			mshr_clear(&(gpu_l2_caches[i].mshrs[j]));
		}*/
	
	/*if(gpu_l2_caches[i].policy_type == 1)
		{
			gpu_l2_caches[i].policy = cache_policy_lru;
		}
		else
		{
			fatal("Invalid cache policy\n");
		}*/
	
	/*l3_caches[i].Tx_queue_top = list_create();
		memset (buff,'\0' , 100);
		snprintf(buff, 100, "l3_caches[%d].Tx_queue_top", i);
		l3_caches[i].Tx_queue_top->name = strdup(buff);*/

		//io ctrl
		/*l3_caches[i].cache_io_up_ec = (void *) calloc((1), sizeof(eventcount));*/
		
		/*memset(buff,'\0' , 100);
		snprintf(buff, 100, "cache_io_up_ec");
		l3_caches[i].cache_io_up_ec = new_eventcount(strdup(buff));*/
		
		//io tasks
		/*l3_caches[i].cache_io_up_tasks = (void *) calloc((1), sizeof(task));*/
		
		/*memset(buff,'\0' , 100);
		snprintf(buff, 100, "cache_io_up_task");
		l3_caches[i].cache_io_up_tasks = create_task(l3_cache_up_io_ctrl, DEFAULT_STACK_SIZE, strdup(buff));*/
	
	/*gpu_v_caches[i].mshrs = (void *) calloc(gpu_v_caches[i].mshr_size, sizeof(struct mshr_t));
		for(j = 0; j < gpu_v_caches[i].mshr_size; j++)
		{
			memset (buff,'\0' , 100);
			snprintf(buff, 100, "gpu_v_caches[%d].mshr[%d]", i, j);
			gpu_v_caches[i].mshrs[j].name = strdup(buff);

			gpu_v_caches[i].mshrs[j].entires = list_create();
			memset (buff,'\0' , 100);
			snprintf(buff, 100, "gpu_v_caches[%d].mshr[%d].entires", i, j);
			gpu_v_caches[i].mshrs[j].entires->name = strdup(buff);

			mshr_clear(&(gpu_v_caches[i].mshrs[j]));
		}*/
	
	/*if(gpu_v_caches[i].policy_type == 1)
		{
			gpu_v_caches[i].policy = cache_policy_lru;
		}
		else
		{
			fatal("Invalid cache policy\n");
		}*/
	
	/*l3_caches[i].Tx_queue_top = list_create();
		memset (buff,'\0' , 100);
		snprintf(buff, 100, "l3_caches[%d].Tx_queue_top", i);
		l3_caches[i].Tx_queue_top->name = strdup(buff);*/
		
		//io ctrl
		/*l3_caches[i].cache_io_up_ec = (void *) calloc((1), sizeof(eventcount));*/

		/*memset(buff,'\0' , 100);
		snprintf(buff, 100, "cache_io_up_ec");
		l3_caches[i].cache_io_up_ec = new_eventcount(strdup(buff));*/

		//io tasks
		/*l3_caches[i].cache_io_up_tasks = (void *) calloc((1), sizeof(task));*/

		/*memset(buff,'\0' , 100);
		snprintf(buff, 100, "cache_io_up_task");
		l3_caches[i].cache_io_up_tasks = create_task(l3_cache_up_io_ctrl, DEFAULT_STACK_SIZE, strdup(buff));*/
	
	/*gpu_s_caches[i].mshrs = (void *) calloc(gpu_s_caches[i].mshr_size, sizeof(struct mshr_t));
		for(j = 0; j < gpu_s_caches[i].mshr_size; j++)
		{
			memset (buff,'\0' , 100);
			snprintf(buff, 100, "gpu_s_caches[%d].mshr[%d]", i, j);
			gpu_s_caches[i].mshrs[j].name = strdup(buff);

			gpu_s_caches[i].mshrs[j].entires = list_create();
			memset (buff,'\0' , 100);
			snprintf(buff, 100, "gpu_s_caches[%d].mshr[%d].entires", i, j);
			gpu_s_caches[i].mshrs[j].entires->name = strdup(buff);

			mshr_clear(&(gpu_s_caches[i].mshrs[j]));
		}*/
	
		/*if(gpu_s_caches[i].policy_type == 1)
		{
			gpu_s_caches[i].policy = cache_policy_lru;
		}
		else
		{
			fatal("Invalid cache policy\n");
		}*/
	
	/*CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu l2 queue free size %d\n",
			cache->name, message_packet->access_id, P_TIME, list_count(l2_caches[cache->id].Rx_queue_top));
	CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
			cache->name, message_packet->access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));
	CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu %s as %s\n",
			cache->name, message_packet->access_id, P_TIME, l2_caches[cache->id].name, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));
	CGM_DEBUG(protocol_debug_file, "%s Access_id %llu cycle %llu %s miss SEND %s %s\n",
			cache->name, message_packet->access_id, P_TIME, cache->name, l2_caches[cache->id].name, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));*/
	
	void cgm_mesi_l2_get(struct cache_t *cache, struct cgm_packet_t *message_packet){

	enum cgm_access_kind_t access_type;
	/*long long access_id = 0;*/
	int cache_block_hit;
	int cache_block_state;
	int *cache_block_hit_ptr = &cache_block_hit;
	int *cache_block_state_ptr = &cache_block_state;

	access_type = message_packet->access_type;
	/*access_id = message_packet->access_id;*/

	int l3_map;

	//get the status of the cache block
	cache_get_block_status(&(l2_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

	switch(*cache_block_state_ptr)
	{
		case cgm_cache_block_noncoherent:
		case cgm_cache_block_owned:
			fatal("l2_cache_ctrl(): Invalid block state on load hit as %s cycle %llu\n",
					str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr), P_TIME);
			break;

		case cgm_cache_block_invalid:

			/*printf("l2 get miss\n");*/

			//stats
			l2_caches[my_pid].misses++;

			//check ORT for coalesce
			cache_check_ORT(&(l2_caches[my_pid]), message_packet);

			if(message_packet->coalesced == 1)
				continue;

			//error checking check this in L3 cache
			//message_packet->cache_block_state = *cache_block_hit_ptr;

			//add some routing/status data to the packet
			message_packet->access_type = cgm_access_get;

			l3_map = cgm_l3_cache_map(message_packet->set);
			message_packet->l2_cache_id = l2_caches[my_pid].id;
			message_packet->l2_cache_name = str_map_value(&l2_strn_map, l2_caches[my_pid].id);

			message_packet->src_name = l2_caches[my_pid].name;
			message_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);
			message_packet->dest_name = l3_caches[l3_map].name;
			message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

			// we are bringing a new block so evict the victim and flush the L1 copies
			//find victim
			message_packet->l2_victim_way = cgm_cache_replace_block(&(l2_caches[my_pid]), message_packet->set);
			cgm_L2_cache_evict_block(&(l2_caches[my_pid]), message_packet->set, message_packet->l2_victim_way);

			//charge delay
			P_PAUSE(l2_caches[my_pid].latency);

			//transmit to L3
			cache_put_io_down_queue(&(l2_caches[my_pid]), message_packet);
			break;

		case cgm_cache_block_modified:
		case cgm_cache_block_exclusive:
		case cgm_cache_block_shared:

			P_PAUSE(l2_caches[my_pid].latency);

			//set message size
			message_packet->size = l1_d_caches[my_pid].block_size; //this can be either L1 I or L1 D cache block size.

			//update message status
			if(*cache_block_state_ptr == cgm_cache_block_modified)
			{
				message_packet->access_type = cgm_access_putx;
			}
			else if(*cache_block_state_ptr == cgm_cache_block_exclusive)
			{
				message_packet->access_type = cgm_access_put_clnx;
			}
			else if(*cache_block_state_ptr == cgm_cache_block_shared)
			{
				message_packet->access_type = cgm_access_puts;
			}

			/*this will send the block and block state up to the high level cache.*/
			message_packet->cache_block_state = *cache_block_state_ptr;
			/*assert(*cache_block_state_ptr == cgm_cache_block_shared);*/

			cache_put_io_up_queue(&(l2_caches[my_pid]), message_packet);

			if(access_type == cgm_access_load_retry || message_packet->coalesced == 1)
			{
				//enter retry state.
				cache_coalesed_retry(&(l2_caches[my_pid]), message_packet->tag, message_packet->set);
			}

			break;
	}
}
	
	
	void cgm_mesi_fetch(struct cache_t *cache, struct cgm_packet_t *message_packet){

	enum cgm_access_kind_t access_type;
	/*long long access_id = 0;*/
	int cache_block_hit;
	int cache_block_state;
	int *cache_block_hit_ptr = &cache_block_hit;
	int *cache_block_state_ptr = &cache_block_state;

	access_type = message_packet->access_type;
	/*access_id = message_packet->access_id;*/

	//get the status of the cache block
	/*cache_get_block_status(&(l1_i_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);*/
	cache_get_block_status(cache, message_packet, cache_block_hit_ptr, cache_block_state_ptr);

	switch(*cache_block_state_ptr)
	{
		case cgm_cache_block_noncoherent:
		case cgm_cache_block_modified:
		case cgm_cache_block_owned:
			fatal("l1_i_cache_ctrl(): Invalid block state on hit as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
			break;

		//miss or invalid cache block states
		case cgm_cache_block_invalid:

			/*printf("I$ miss fetch\n");*/

			//stats
			cache->misses++;
			/*l1_i_caches[my_pid].misses++;*/

			//check ORT for coalesce
			/*cache_check_ORT(&(l1_i_caches[my_pid]), message_packet);*/
			cache_check_ORT(cache, message_packet);

			if(message_packet->coalesced == 1)
				return;
				/*continue;*/

			//add some routing/status data to the packet
			//message_packet->cpu_access_type = cgm_access_fetch;
			message_packet->access_type = cgm_access_gets;
			message_packet->l1_access_type = cgm_access_gets;

			//find victim and evict on return l1_i_cache just drops the block on return
			/*message_packet->l1_victim_way = cgm_cache_replace_block(&(l1_i_caches[my_pid]), message_packet->set);*/
			message_packet->l1_victim_way = cgm_cache_replace_block(cache, message_packet->set);

			//charge delay
			/*P_PAUSE(l1_i_caches[my_pid].latency);*/
			P_PAUSE(cache->latency);

			//transmit to L2
			/*cache_put_io_down_queue(&(l1_i_caches[my_pid]), message_packet);*/
			cache_put_io_down_queue(cache, message_packet);
			break;

		//hit states todo fix the exclusive state
		case cgm_cache_block_exclusive:
		case cgm_cache_block_shared:

			//stats
			cache->hits++;
			/*l1_i_caches[my_pid].hits++;*/

			//set retry state and delay
			if(access_type == cgm_access_fetch_retry || message_packet->coalesced == 1)
			{
				/*P_PAUSE(l1_i_caches[my_pid].latency);*/
				P_PAUSE(cache->latency);

				//enter retry state.
				/*cache_coalesed_retry(&(l1_i_caches[my_pid]), message_packet->tag, message_packet->set);*/
				cache_coalesed_retry(cache, message_packet->tag, message_packet->set);
			}

			/*printf("fetch id %llu set %d tag %d complete cycle %llu\n", message_packet->access_id, message_packet->tag, message_packet->set, P_TIME);*/
			message_packet->end_cycle = P_TIME;
			/*cache_l1_i_return(&(l1_i_caches[my_pid]),message_packet);*/
			cache_l1_i_return(cache,message_packet);
			break;
	}

	return;
}
	
	/*star todo figure out if we want to go in this direction.
				if(cgm_cache_protocol == cgm_protocol_mesi)
				{
					cgm_mesi_l3_gets(&(l3_caches[my_pid]), message_packet);
				}
				else
				{
					fatal("l1_d_cache_ctrl(): invalid protocol\n");
				}*/
	
	/*star todo figure out if we want to go in this direction.
				if(cgm_cache_protocol == cgm_protocol_mesi)
				{
					cgm_mesi_l2_gets(&(l2_caches[my_pid]), message_packet);
				}
				else
				{
					fatal("l1_d_cache_ctrl(): invalid protocol\n");
				}*/
	
	/*star todo figure out if we want to go in this direction.
				if(cgm_cache_protocol == cgm_protocol_mesi)
				{
					cgm_mesi_load(&(l1_d_caches[my_pid]), message_packet);
				}
				else
				{
					fatal("l1_d_cache_ctrl(): invalid protocol\n");
				}*/
	
	/*star todo figure out if we want to go in this direction.
				if(cgm_cache_protocol == cgm_protocol_mesi)
				{
					cgm_mesi_store(&(l1_d_caches[my_pid]), message_packet);
				}
				else
				{
					fatal("l1_d_cache_ctrl(): invalid protocol\n");
				}*/
	
	/*if(cgm_cache_protocol == cgm_protocol_mesi)
				{
					cgm_mesi_l1_i_puts(&(l1_i_caches[my_pid]), message_packet);
				}
				else
				{
					fatal("l1_i_cache_ctrl(): invalid protocol\n");
				}*/
	
	//star todo figure out if we want to go in this direction.

				//via call back functions

				/*if(cgm_cache_protocol == cgm_protocol_mesi)
				{
					//via regualr functions
					//cgm_mesi_fetch(&(l1_i_caches[my_pid]), message_packet);
				}
				else
				{
					fatal("l1_i_cache_ctrl(): invalid protocol\n");
				}*/
	
	void cgm_mesi_l3_gets(struct cache_t *cache, struct cgm_packet_t *message_packet){

	enum cgm_access_kind_t access_type;
	/*long long access_id = 0;*/
	int cache_block_hit;
	int cache_block_state;
	int *cache_block_hit_ptr = &cache_block_hit;
	int *cache_block_state_ptr = &cache_block_state;

	access_type = message_packet->access_type;
	/*access_id = message_packet->access_id;*/

	int l3_map;

	//get the status of the cache block
	cache_get_block_status(&(l3_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

	//check the directory dirty bit status for sanity check
	dirty = cgm_cache_get_dir_dirty_bit(&(l3_caches[my_pid]), message_packet->set, message_packet->way);

	switch(*cache_block_state_ptr)
	{
		case cgm_cache_block_noncoherent:
		case cgm_cache_block_modified:
		case cgm_cache_block_owned:
			/*printf("l3_cache_ctrl(): GetS invalid block state on hit as %d\n", *cache_block_state_ptr);*/
			fatal("l3_cache_ctrl(): GetS invalid block state on hit as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
			break;

		case cgm_cache_block_invalid:

			//stats;
			l3_caches[my_pid].misses++;

			//check ORT for coalesce
			cache_check_ORT(&(l3_caches[my_pid]), message_packet);

			if(message_packet->coalesced == 1)
				continue;

			//find victim again because LRU has been updated on hits.
			message_packet->l3_victim_way = cgm_cache_replace_block(&(l3_caches[my_pid]), message_packet->set);

			//add some routing/status data to the packet
			message_packet->access_type = cgm_access_mc_get;

			//set return cache block state
			//star todo look into this, this should at least works for I$ requests
			message_packet->cache_block_state = cgm_cache_block_shared;

			assert(message_packet->cpu_access_type == cgm_access_fetch);

			message_packet->src_name = l3_caches[my_pid].name;
			message_packet->src_id = str_map_string(&node_strn_map, l3_caches[my_pid].name);
			message_packet->dest_id = str_map_string(&node_strn_map, "sys_agent");
			message_packet->dest_name = str_map_value(&node_strn_map, message_packet->dest_id);

			//charge delay
			P_PAUSE(l3_caches[my_pid].latency);

			//transmit to SA/MC
			cache_put_io_down_queue(&(l3_caches[my_pid]), message_packet);
			break;

		case cgm_cache_block_exclusive:
		case cgm_cache_block_shared:

			//stats;
			l3_caches[my_pid].hits++;

			//GetS should never be dirty in directory
			assert(dirty == 0);

			//set the presence bit in the directory for the requesting core.
			cgm_cache_set_dir(&(l3_caches[my_pid]), message_packet->set, message_packet->way, message_packet->l2_cache_id);

			//update message packet status
			message_packet->size = l2_caches[str_map_string(&node_strn_map, message_packet->l2_cache_name)].block_size;
			message_packet->access_type = cgm_access_puts;

			/*star todo this is broken, try to fix this.
			the I$ is accessing memory in the D$'s swim lane*/
			if(*cache_block_state_ptr == cgm_cache_block_exclusive)
			{
				message_packet->cache_block_state = cgm_cache_block_shared;
			}
			else
			{
				message_packet->cache_block_state = *cache_block_state_ptr;
			}

			//message_packet->cache_block_state = cache_block_shared;
			//assert(message_packet->cache_block_state == cache_block_shared);
			/*printf("l3 block type %s\n", str_map_value(&cache_block_state_map, *cache_block_state_ptr));*/

			message_packet->dest_id = str_map_string(&node_strn_map, message_packet->l2_cache_name);
			message_packet->dest_name = str_map_value(&l2_strn_map, message_packet->dest_id);
			message_packet->src_name = l3_caches[my_pid].name;
			message_packet->src_id = str_map_string(&node_strn_map, l3_caches[my_pid].name);

			P_PAUSE(l3_caches[my_pid].latency);

			//printf("Sending %s\n", str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

			cache_put_io_up_queue(&(l3_caches[my_pid]), message_packet);

			//check if the packet has coalesced accesses.
			if(access_type == cgm_access_fetch_retry || message_packet->coalesced == 1)
			{
				//enter retry state.
				cache_coalesed_retry(&(l3_caches[my_pid]), message_packet->tag, message_packet->set);
			}
			break;
	}







	return;
}
	
	enum cgm_access_kind_t access_type;
	/*long long access_id = 0;*/
	int cache_block_hit;
	int cache_block_state;
	int *cache_block_hit_ptr = &cache_block_hit;
	int *cache_block_state_ptr = &cache_block_state;

	access_type = message_packet->access_type;
	/*access_id = message_packet->access_id;*/

	//get the status of the cache block
	cache_get_block_status(&(l2_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

	switch(*cache_block_state_ptr)
	{

		case cgm_cache_block_noncoherent:
		case cgm_cache_block_modified:
		case cgm_cache_block_owned:
			fatal("l2_cache_ctrl(): Invalid block state on fetch hit as %s cycle %llu\n",
					str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr), P_TIME);
			break;

		case cgm_cache_block_invalid:

			//stats
			l2_caches[my_pid].misses++;

			//check ORT for coalesce
			cache_check_ORT(&(l2_caches[my_pid]), message_packet);

			if(message_packet->coalesced == 1)
				continue;

			//add some routing/status data to the packet
			message_packet->access_type = cgm_access_gets;

			/*message_packet->cache_block_state = *cache_block_hit_ptr;*/

			l3_map = cgm_l3_cache_map(message_packet->set);
			message_packet->l2_cache_id = l2_caches[my_pid].id;
			message_packet->l2_cache_name = str_map_value(&l2_strn_map, l2_caches[my_pid].id);

			message_packet->src_name = l2_caches[my_pid].name;
			message_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);
			message_packet->dest_name = l3_caches[l3_map].name;
			message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

			//find victim, on return OK to just drop the block this is I$ traffic
			message_packet->l2_victim_way = cgm_cache_replace_block(&(l2_caches[my_pid]), message_packet->set);
			assert(message_packet->l2_victim_way >= 0 && message_packet->l2_victim_way < l2_caches[my_pid].assoc);

			//charge delay
			P_PAUSE(l2_caches[my_pid].latency);

			//transmit to L3
			cache_put_io_down_queue(&(l2_caches[my_pid]), message_packet);
			break;


		//star todo fix this exclusive bug
		case cgm_cache_block_exclusive:
		case cgm_cache_block_shared:

			//stats
			l2_caches[my_pid].hits++;

			/*if this fails the I$ has tried to accesses a block in the D$ swim lane.*/
			/*assert(*cache_block_state_ptr != cgm_cache_block_exclusive);*/

			/*star todo this is broken, try to fix this.
			the I$ is accessing memory in the D$'s swim lane*/
			if(*cache_block_state_ptr == cgm_cache_block_exclusive)
			{
				message_packet->cache_block_state = cgm_cache_block_shared;
				//printf("I$ block found exclusive in L2 cycle %llu\n", P_TIME);
			}
			else
			{
				message_packet->cache_block_state = *cache_block_state_ptr;
			}
			//star todo end fix it code here. Delete the code up above if the error goes away

			P_PAUSE(l2_caches[my_pid].latency);

			//set message size
			message_packet->size = l1_i_caches[my_pid].block_size; //this can be either L1 I or L1 D cache block size.

			//update message status
			message_packet->access_type = cgm_access_puts;
			message_packet->cache_block_state = *cache_block_state_ptr;

			cache_put_io_up_queue(&(l2_caches[my_pid]), message_packet);

			//check if the packet has coalesced accesses.
			if(access_type == cgm_access_fetch_retry || message_packet->coalesced == 1)
			{
				//enter retry state.
				cache_coalesed_retry(&(l2_caches[my_pid]), message_packet->tag, message_packet->set);
			}

			break;
	}
	
	//get the status of the cache block
	cache_get_block_status(&(l1_d_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

	switch(*cache_block_state_ptr)
	{
		case cgm_cache_block_owned:
		case cgm_cache_block_noncoherent:
			fatal("l1_d_cache_ctrl(): Invalid block state on store hit %s \n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
			break;

		//miss or invalid cache block state
		case cgm_cache_block_invalid:

			//stats
			l1_d_caches[my_pid].misses++;

			//check ORT for coalesce
			cache_check_ORT(&(l1_d_caches[my_pid]), message_packet);

			if(message_packet->coalesced == 1)
				continue;

			/*printf("store (inval) access_id %llu set %d tag %d as %s coalesced cycle %llu\n",
				message_packet->access_id, message_packet->set, message_packet->tag, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);*/

			//add some routing/status data to the packet
			message_packet->access_type = cgm_access_getx;
			message_packet->l1_access_type = cgm_access_getx;

			//find victim
			message_packet->l1_victim_way = cgm_cache_replace_block(&(l1_d_caches[my_pid]), message_packet->set);

			//charge delay
			P_PAUSE(l1_d_caches[my_pid].latency);

			//transmit to L2
			cache_put_io_down_queue(&(l1_d_caches[my_pid]), message_packet);
			break;


		case cgm_cache_block_shared:

			//stats
			l1_d_caches[my_pid].upgrade_misses++;

			//should not be here in the retry state.
			//assert(message_packet->access_type != cgm_access_store_retry);

			/*star todo find a better way to do this.
			this is for a special case where a coalesced store
			can be pulled from the ORT and is an upgrade miss here
			at this point we want the access to be treated as a new miss
			so set coalesced to 0. packets in the ORT will stay in the ort
			preserving order until the missing access returns with the upgrade.*/
			if(message_packet->coalesced == 1)
			{
				message_packet->coalesced = 0;
			}

			//check ORT for coalesce
			cache_check_ORT(&(l1_d_caches[my_pid]), message_packet);

			if(message_packet->coalesced == 1)
				continue;

			/*printf("store (share) access_id %llu set %d tag %d as %s coalesced cycle %llu\n",
				message_packet->access_id, message_packet->set, message_packet->tag, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);*/

			//set block transient state
			cgm_cache_set_block_transient_state(&(l1_d_caches[my_pid]), message_packet->set, message_packet->way, message_packet->access_id, cgm_cache_block_transient);

			message_packet->access_type = cgm_access_upgrade;

			//charge delay
			P_PAUSE(l1_d_caches[my_pid].latency);

			//transmit upgrade request to L2
			//printf("access_id %llu forwarded set %d tag %d cycle %llu\n", message_packet->access_id, message_packet->set, message_packet->tag, P_TIME);
			cache_put_io_down_queue(&(l1_d_caches[my_pid]), message_packet);
			break;

		case cgm_cache_block_exclusive:
		case cgm_cache_block_modified:

			l1_d_caches[my_pid].hits++;

			//set modified if current block state is exclusive
			if(*cache_block_state_ptr == cgm_cache_block_exclusive)
			{
				cgm_cache_set_block_state(&(l1_d_caches[my_pid]), message_packet->set, message_packet->way, cgm_cache_block_modified);
			}

			//check for retry state
			if(access_type == cgm_access_store_retry || message_packet->coalesced == 1)
			{
				P_PAUSE(l1_d_caches[my_pid].latency);

				//enter retry state.
				cache_coalesed_retry(&(l1_d_caches[my_pid]), message_packet->tag, message_packet->set);
			}

			message_packet->end_cycle = P_TIME;
			cache_l1_d_return(&(l1_d_caches[my_pid]),message_packet);
			break;
	}

void run_array(void);

array[(int)message_packet->access_id] = 0;*/

int i = 0;
	for(i = 0; i <10000; i++)
	{
		array[i] = 0;
	}
	
int array[10000];

void run_array(void){


	int i = 0;

	printf("running access array\n");

	for(i = 0; i < 10000; i ++)
	{
		if(array[i] == 1)
		{
			printf("access_id %d failed\n", i);
		}

	}

}

/*printf("access_id %llu L2 putting cycle %llu \n", message_packet->access_id, P_TIME);
		printf("size %d \n", list_count(cache->last_queue));

		//star todo add the wb back in.

		//evict the victim
		//the block can be dropped if it is not modified.

		//victim_state = cgm_cache_get_block_state(cache, message_packet->set, message_packet->l2_victim_way);

		first if the block is modified it is dirty and needs to be written back
		move a copy of the block to the write back buffer
		if (victim_state == cgm_cache_block_modified)
		{
			//move the block to the WB buffer
			struct cgm_packet_t *write_back_packet = packet_create();

			//star todo remember to set l2 cache id in WB packet
			init_write_back_packet(cache, write_back_packet, message_packet->set, message_packet->l2_victim_way);

			list_enqueue(cache->write_back_buffer, write_back_packet);
		}*/

//checks
		if(message_packet->access_type == cgm_access_put_clnx ||message_packet->access_type == cgm_access_putx)
		{
			assert(message_packet->cache_block_state == cgm_cache_block_exclusive || message_packet->cache_block_state == cgm_cache_block_modified);
		}
		else if(message_packet->access_type == cgm_access_puts)
		{
			assert(message_packet->cache_block_state == cgm_cache_block_shared);
		}

/*victim_state = cgm_cache_get_block_state(cache, message_packet->set, message_packet->l1_victim_way);

		first if the block is modified it is dirty and needs to be written back
		move a copy of the block to the write back buffer
		if (victim_state == cgm_cache_block_modified)
		{
			//move the block to the WB buffer
			struct cgm_packet_t *write_back_packet = packet_create();

			init_write_back_packet(cache, write_back_packet, message_packet->set, message_packet->l1_victim_way);

			list_enqueue(cache->write_back_buffer, write_back_packet);
		}*/

/*printf("l1_i_cache retry queue size %d\n", list_count(l1_i_caches[0].retry_queue));
	printf("l1_i_cache Rx_top queue size %d\n", list_count(l1_i_caches[0].Rx_queue_top));
	printf("l1_i_cache Rx_bottom queue size %d\n", list_count(l1_i_caches[0].Rx_queue_bottom));

	printf("l1_d_cache retry queue size %d\n", list_count(l1_d_caches[0].retry_queue));
	printf("l1_d_cache Rx_top queue size %d\n", list_count(l1_d_caches[0].Rx_queue_top));
	printf("l1_d_cache Rx_bottom queue size %d\n", list_count(l1_d_caches[0].Rx_queue_bottom));

	printf("l2_cache retry queue size %d\n", list_count(l2_caches[0].retry_queue));
	printf("l2_cache Rx_top queue size %d\n", list_count(l2_caches[0].Rx_queue_top));
	printf("l2_cache Rx_bottom queue size %d\n", list_count(l2_caches[0].Rx_queue_bottom));

	struct cgm_packet_t *mp = list_get(l1_d_caches[0].retry_queue, 0);
	printf("rt access_id %llu\n", mp->access_id);
	mp = list_get(l1_d_caches[0].Rx_queue_top, 0);
	printf("top access_id %llu\n", mp->access_id);

	STOP;*/

//miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{


					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:
						case cgm_cache_block_exclusive:
							fatal("l3_cache_ctrl(): Invalid block state on miss\n");
							break;

						case cgm_cache_block_invalid:
						case cgm_cache_block_shared:

							//check ORT for coalesce
							cache_check_ORT(&(l3_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;

							//find victim again because LRU has been updated on hits.
							message_packet->l3_victim_way = cgm_cache_replace_block(&(l3_caches[my_pid]), message_packet->set);

							//add some routing/status data to the packet
							message_packet->access_type = cgm_access_mc_get;
							message_packet->cache_block_state = cgm_cache_block_exclusive;

							message_packet->src_name = l3_caches[my_pid].name;
							message_packet->src_id = str_map_string(&node_strn_map, l3_caches[my_pid].name);
							message_packet->dest_id = str_map_string(&node_strn_map, "sys_agent");
							message_packet->dest_name = str_map_value(&node_strn_map, message_packet->dest_id);

							//charge delay
							P_PAUSE(l3_caches[my_pid].latency);

							//transmit to L3
							cache_put_io_down_queue(&(l3_caches[my_pid]), message_packet);
							break;
					}
				}
			}

//miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{

					l3_caches[my_pid].misses++;

					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:
						case cgm_cache_block_exclusive:
						case cgm_cache_block_shared:
							fatal("l3_cache_ctrl(): Invalid block state on miss as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
							break;

						case cgm_cache_block_invalid:

							//check ORT for coalesce
							cache_check_ORT(&(l3_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;

							//find victim again because LRU has been updated on hits.
							message_packet->l3_victim_way = cgm_cache_replace_block(&(l3_caches[my_pid]), message_packet->set);

							//add some routing/status data to the packet
							message_packet->access_type = cgm_access_mc_get;

							message_packet->cache_block_state = cgm_cache_block_exclusive;

							//set the data type bit in the block
							assert(message_packet->cpu_access_type == cgm_access_load);

							/*int type;
							type = message_packet->cpu_access_type == cgm_access_fetch ? 1 : 0;
							cgm_cache_set_block_type(&(l3_caches[my_pid]), type, message_packet->set, message_packet->l3_victim_way);*/

							message_packet->src_name = l3_caches[my_pid].name;
							message_packet->src_id = str_map_string(&node_strn_map, l3_caches[my_pid].name);
							message_packet->dest_id = str_map_string(&node_strn_map, "sys_agent");
							message_packet->dest_name = str_map_value(&node_strn_map, message_packet->dest_id);

							//charge delay
							P_PAUSE(l3_caches[my_pid].latency);

							//transmit to SA/MC
							cache_put_io_down_queue(&(l3_caches[my_pid]), message_packet);
							break;
					}
				}
			}

//miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{
					l3_caches[my_pid].misses++;

					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:
						case cgm_cache_block_exclusive:
						case cgm_cache_block_shared:
							fatal("l3_cache_ctrl(): Invalid block state on miss as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
							break;


						case cgm_cache_block_invalid:


							//check ORT for coalesce
							cache_check_ORT(&(l3_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;

							//find victim again because LRU has been updated on hits.
							message_packet->l3_victim_way = cgm_cache_replace_block(&(l3_caches[my_pid]), message_packet->set);

							//add some routing/status data to the packet
							message_packet->access_type = cgm_access_mc_get;

							message_packet->cache_block_state = cgm_cache_block_shared;

							//set the data type bit in the block
							assert(message_packet->cpu_access_type == cgm_access_fetch);

							/*int type;
							type = message_packet->cpu_access_type == cgm_access_fetch ? 1 : 0;
							cgm_cache_set_block_type(&(l3_caches[my_pid]), type, message_packet->set, message_packet->l3_victim_way);*/

							message_packet->src_name = l3_caches[my_pid].name;
							message_packet->src_id = str_map_string(&node_strn_map, l3_caches[my_pid].name);
							message_packet->dest_id = str_map_string(&node_strn_map, "sys_agent");
							message_packet->dest_name = str_map_value(&node_strn_map, message_packet->dest_id);

							//charge delay
							P_PAUSE(l3_caches[my_pid].latency);

							//transmit to SA/MC
							cache_put_io_down_queue(&(l3_caches[my_pid]), message_packet);
							break;
					}
				}
			}

//Miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{


					/*printf("l2 GetX access miss\n");
					STOP;*/

					assert(*cache_block_state_ptr == cgm_cache_block_invalid);

					switch(*cache_block_state_ptr)
					{
						default:
							fatal("L3 bad cache block state\n");
							break;

						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:
						case cgm_cache_block_exclusive:
						case cgm_cache_block_shared:
							fatal("l2_cache_ctrl(): Invalid block state on store miss as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
							break;

						case cgm_cache_block_invalid:

							//check L1 block state
							assert(message_packet->cache_block_state == cgm_cache_block_invalid);

							//check ORT for coalesce
							cache_check_ORT(&(l2_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;

							//find victim
							message_packet->l2_victim_way = cgm_cache_replace_block(&(l2_caches[my_pid]), message_packet->set);

							//set the data type bit in the block
							int type;
							type = message_packet->cpu_access_type == cgm_access_fetch ? 1 : 0;
							cgm_cache_set_block_type(&(l2_caches[my_pid]), type, message_packet->set, message_packet->l2_victim_way);

							//set access type
							message_packet->access_type = cgm_access_getx;

							//set L2 block state
							message_packet->cache_block_state = *cache_block_state_ptr;

							//set routing to the packet
							int l3_map;
							l3_map = cgm_l3_cache_map(message_packet->set);
							message_packet->l2_cache_id = l2_caches[my_pid].id;
							message_packet->l2_cache_name = str_map_value(&l2_strn_map, l2_caches[my_pid].id);

							message_packet->src_name = l2_caches[my_pid].name;
							message_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);
							message_packet->dest_name = l3_caches[l3_map].name;
							message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

							//charge delay
							P_PAUSE(l2_caches[my_pid].latency);

							//transmit to L3
							cache_put_io_down_queue(&(l2_caches[my_pid]), message_packet);

							//printf("L2 transmitting GetX as %s cycle %llu\n", str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
							break;

					}
				}
			}


//Miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{


					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:
						case cgm_cache_block_exclusive:
						case cgm_cache_block_shared:
							fatal("l2_cache_ctrl(): Invalid block state on load miss as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
							break;

						case cgm_cache_block_invalid:

							//check ORT for coalesce
							cache_check_ORT(&(l2_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;

							//error checking check this in L3 cache
							//message_packet->cache_block_state = *cache_block_hit_ptr;

							//add some routing/status data to the packet
							message_packet->access_type = cgm_access_get;

							int l3_map;
							l3_map = cgm_l3_cache_map(message_packet->set);

							message_packet->l2_cache_id = l2_caches[my_pid].id;
							message_packet->l2_cache_name = str_map_value(&l2_strn_map, l2_caches[my_pid].id);

							message_packet->src_name = l2_caches[my_pid].name;
							message_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);
							message_packet->dest_name = l3_caches[l3_map].name;
							message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

							// we are bringing a new block so evict the victim and invalidate the L1 copies
							//find victim
							message_packet->l2_victim_way = cgm_cache_replace_block(&(l2_caches[my_pid]), message_packet->set);

							//Additions start here
							cgm_cache_evict_block(&(l2_caches[my_pid]), message_packet->set, message_packet->l2_victim_way);
							//addtions stop

							//charge delay
							P_PAUSE(l2_caches[my_pid].latency);

							//transmit to L3
							cache_put_io_down_queue(&(l2_caches[my_pid]), message_packet);
							break;
					}
				}
			}
			

/*//Miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{
					l2_caches[my_pid].misses++;

					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:
						case cgm_cache_block_exclusive:
						case cgm_cache_block_shared:
							fatal("l2_cache_ctrl(): Invalid block state on fetch miss as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
							break;

						case cgm_cache_block_invalid:

							//check ORT for coalesce
							cache_check_ORT(&(l2_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;

							//add some routing/status data to the packet
							message_packet->access_type = cgm_access_gets;

							message_packet->cache_block_state = *cache_block_hit_ptr;

							int l3_map;
							l3_map = cgm_l3_cache_map(message_packet->set);
							message_packet->l2_cache_id = l2_caches[my_pid].id;
							message_packet->l2_cache_name = str_map_value(&l2_strn_map, l2_caches[my_pid].id);

							message_packet->src_name = l2_caches[my_pid].name;
							message_packet->src_id = str_map_string(&node_strn_map, l2_caches[my_pid].name);
							message_packet->dest_name = l3_caches[l3_map].name;
							message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[l3_map].name);

							//find victim
							message_packet->l2_victim_way = cgm_cache_replace_block(&(l2_caches[my_pid]), message_packet->set);

							//charge delay
							P_PAUSE(l2_caches[my_pid].latency);

							//transmit to L3
							cache_put_io_down_queue(&(l2_caches[my_pid]), message_packet);
							break;
					}
				}*/
			/*}*/

//Miss
				/*else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{
					l1_d_caches[my_pid].misses++;

					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:
						case cgm_cache_block_exclusive:
						case cgm_cache_block_shared:
							fatal("l1_i_cache_ctrl(): Invalid block state on miss\n");
							break;

						case cgm_cache_block_invalid:

							printf("l1 d load miss\n");

							//check ORT for coalesce
							cache_check_ORT(&(l1_d_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;

							printf("load access_id %llu set %d tag %d as %s coalesced cycle %llu\n",
										message_packet->access_id, message_packet->set, message_packet->tag, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);

							//add some routing/status data to the packet
							message_packet->access_type = cgm_access_get;
							message_packet->l1_access_type = cgm_access_get;

							//find victim
							message_packet->l1_victim_way = cgm_cache_replace_block(&(l1_d_caches[my_pid]), message_packet->set);

							//charge delay
							P_PAUSE(l1_d_caches[my_pid].latency);

							//transmit to L2
							cache_put_io_down_queue(&(l1_d_caches[my_pid]), message_packet);
							break;
					}
				}*/
			/*}*/

//Miss
				/*else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{
					l1_d_caches[my_pid].misses++;

					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_exclusive:
						case cgm_cache_block_owned:
						case cgm_cache_block_modified:
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_shared:
							fatal("l1_i_cache_ctrl(): Invalid block state on miss\n");
							break;

						case cgm_cache_block_invalid:

							//check ORT for coalesce

							//printf("L1 D store miss\n");

							cache_check_ORT(&(l1_d_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;
							printf("store (inval) access_id %llu set %d tag %d as %s coalesced cycle %llu\n",
								message_packet->access_id, message_packet->set, message_packet->tag, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);

							//add some routing/status data to the packet
							message_packet->access_type = cgm_access_getx;
							message_packet->l1_access_type = cgm_access_getx;

							//find victim
							message_packet->l1_victim_way = cgm_cache_replace_block(&(l1_d_caches[my_pid]), message_packet->set);

							//charge delay
							P_PAUSE(l1_d_caches[my_pid].latency);

							//transmit to L2
							cache_put_io_down_queue(&(l1_d_caches[my_pid]), message_packet);
							break;
					}
				}*/
			/*}*/

/*//hit
				if(*cache_block_hit_ptr && *cache_block_state_ptr != cgm_cache_block_invalid)
				{
					//stats;
					l1_i_caches[my_pid].hits++;

					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_invalid:
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:

							fatal("l1_i_cache_ctrl(): Invalid block state on hit as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
							break;

						case cgm_cache_block_exclusive:
						case cgm_cache_block_shared:

							//set retry state and delay
							if(access_type == cgm_access_fetch_retry || message_packet->coalesced == 1)
							{
								P_PAUSE(l1_i_caches[my_pid].latency);

								//enter retry state.
								cache_coalesed_retry(&(l1_i_caches[my_pid]), message_packet->tag, message_packet->set);
							}

							printf("fetch id %llu set %d tag %d complete cycle %llu\n", message_packet->access_id, message_packet->tag, message_packet->set, P_TIME);
							message_packet->end_cycle = P_TIME;
							cache_l1_i_return(&(l1_i_caches[my_pid]),message_packet);
							break;
					}
				}
				//Miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cgm_cache_block_invalid)
				{
					l1_i_caches[my_pid].misses++;

					switch(*cache_block_state_ptr)
					{
						case cgm_cache_block_noncoherent:
						case cgm_cache_block_modified:
						case cgm_cache_block_owned:
						case cgm_cache_block_exclusive:
						case cgm_cache_block_shared:
							fatal("l1_i_cache_ctrl(): GetS invalid block state on miss as %s\n", str_map_value(&cgm_cache_block_state_map, *cache_block_state_ptr));
							break;

						case cgm_cache_block_invalid:

							//check ORT for coalesce
							cache_check_ORT(&(l1_i_caches[my_pid]), message_packet);

							if(message_packet->coalesced == 1)
								continue;

							//add some routing/status data to the packet
							//message_packet->cpu_access_type = cgm_access_fetch;
							message_packet->access_type = cgm_access_gets;
							message_packet->l1_access_type = cgm_access_gets;

							//find victim
							message_packet->l1_victim_way = cgm_cache_replace_block(&(l1_i_caches[my_pid]), message_packet->set);

							//charge delay
							P_PAUSE(l1_i_caches[my_pid].latency);

							//transmit to L2
							cache_put_io_down_queue(&(l1_i_caches[my_pid]), message_packet);
							break;
					}
				}*/

//get the block type bit from the victim
		/*type = cgm_cache_get_block_type(cache, message_packet->set, message_packet->l2_victim_way, message_packet->tag);

		if(type == 1) //instruction cache data
		{
			//write the block
			cgm_cache_set_block(cache, message_packet->set, message_packet->l2_victim_way, message_packet->tag, message_packet->cache_block_state);

			//set retry state
			assert(message_packet->cpu_access_type == cgm_access_fetch);
			message_packet->access_type = cgm_cache_get_retry_state(message_packet->cpu_access_type);
		}
		else if(type == 0) //data cache data
		{
			//write the block
			//star todo need detail here.
			cgm_cache_set_block(cache, message_packet->set, message_packet->l2_victim_way, message_packet->tag, message_packet->cache_block_state);
			assert(message_packet->cpu_access_type == cgm_access_load || message_packet->cpu_access_type == cgm_access_store);
			message_packet->access_type = cgm_cache_get_retry_state(message_packet->cpu_access_type);
		}*/

//drop the packet into the sys agent queue
						//list_enqueue(system_agent->Rx_queue_top, message_packet);
						//advance(system_agent_ec);

/*while(!cache_can_access_bottom(&l2_caches[my_pid]))
					{
						printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);//the L2 cache queue is full try again next cycle
						P_PAUSE(1);
					}*/

//drop the packet into the cache's queue
						//list_enqueue(l2_caches[my_pid].Rx_queue_bottom, message_packet);
						//future_advance(&l2_cache[my_pid], WIRE_DELAY(l2_caches[my_pid].wire_latency));
						//advance(&l2_cache[my_pid]);

//drop the packet into the hub's bottom queue
						//list_enqueue(hub_iommu->Rx_queue_bottom, message_packet);
						//advance(hub_iommu_ec);
						//future_advance(hub_iommu_ec, WIRE_DELAY(hub_iommu->wire_latency));

/*while(!hub_iommu_can_access(hub_iommu->Rx_queue_bottom))
					{
						//the hub-iommu cache queue is full try again next cycle
						printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
						P_PAUSE(1);
					}*/

/*while(!cache_can_access_top(&l3_caches[my_pid]))
					{
						//the L2 cache queue is full try again next cycle
						printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
						P_PAUSE(1);
					}*/
					
					

//message_packet->access_type = cgm_access_puts;

							/////////test code
							/*message_packet->access_type = cgm_access_puts;
							message_packet->dest_name = message_packet->src_name;
							message_packet->dest_id = str_map_string(&node_strn_map, message_packet->src_name);
							message_packet->src_name = l3_caches[0].name;
							message_packet->src_id = str_map_string(&node_strn_map, l3_caches[0].name);
							list_enqueue(l2_caches[my_pid].Rx_queue_top, message_packet);
							future_advance(&l2_cache[my_pid], WIRE_DELAY(l2_caches[my_pid].wire_latency));*/
							/////////test code

							//old code
							//drop the packet into the cache's queue
							//list_enqueue(l3_caches[my_pid].Rx_queue_top, message_packet);
							//advance(&l3_cache[my_pid]);
							//future_advance(&l3_cache[my_pid], WIRE_DELAY(l3_caches[my_pid].wire_latency));

/*while(!switch_can_access(switches[my_pid].next_east))
					{
						//the switch queue is full try again next cycle
						printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
						P_PAUSE(1);
					}*/


/*while(!switch_can_access(switches[my_pid].next_west))
					{
						//the switch queue is full try again next cycle
						printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
						P_PAUSE(1);
					}*/


/*while(!switch_can_access(switches[my_pid].next_east))
						{
							//the switch queue is full try again next cycle
							printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
							P_PAUSE(1);
						}*/

/*while(!switch_can_access(switches[my_pid].next_west))
						{
							//the switch queue is full try again next cycle
							printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
							P_PAUSE(1);
						}*/

/*while(!switch_can_access(switches[my_pid].next_west))
						{
							//the switch queue is full try again next cycle
							printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
							P_PAUSE(1);
						}*/

/*while(!switch_can_access(switches[my_pid].next_east))
						{
							//the switch queue is full try again next cycle
							printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
							P_PAUSE(1);
						}*/

/*//get the distance from this switch to the destination (left to right)
					if(dest_node % 3 == 0 && src_node % 3 == 0)
					{
						//L2 to L2
						distance = (dest_node - src_node)/3;
					}
					else if(dest_node % 3 != 0 && src_node % 3 == 0)
					{
						//L2 to L3/SA || L3 to L2 (works for both ways)
						distance = (dest_node - (src_node + 2))/3;
					}
					else
					{
						//L3 to L3/SA
						distance = (dest_node - src_node)/3;
					}*/


/*while(!sys_agent_can_access_top())
					{
						//the sys agent queue is full try again next cycle
						printf("%s stalled access_id %llu cycle %llu\n", switches[my_pid].name, access_id, P_TIME);
						P_PAUSE(1);
					}*/

							//message_packet->access_type = cgm_access_puts;

							/////////test code
							/*message_packet->access_type = cgm_access_puts;
							message_packet->dest_name = message_packet->src_name;
							message_packet->dest_id = str_map_string(&node_strn_map, message_packet->src_name);
							message_packet->src_name = l3_caches[0].name;
							message_packet->src_id = str_map_string(&node_strn_map, l3_caches[0].name);
							list_enqueue(l2_caches[my_pid].Rx_queue_top, message_packet);
							future_advance(&l2_cache[my_pid], WIRE_DELAY(l2_caches[my_pid].wire_latency));*/
							/////////test code

							//old code
							//drop the packet into the cache's queue
							//list_enqueue(l3_caches[my_pid].Rx_queue_bottom, message_packet);
							//advance(&l3_cache[my_pid]);
							
/*l1_i_caches[i].mshrs = (void *) calloc(l1_i_caches[i].mshr_size, sizeof(struct mshr_t));
		for(j = 0; j < l1_i_caches[i].mshr_size; j++)
		{
			memset (buff,'\0' , 100);
			snprintf(buff, 100, "l1_i_caches[%d].mshr[%d]", i, j);
			l1_i_caches[i].mshrs[j].name = strdup(buff);

			l1_i_caches[i].mshrs[j].entires = list_create();

			memset (buff,'\0' , 100);
			snprintf(buff, 100, "l1_i_caches[%d].mshr[%d].entires", i, j);
			l1_i_caches[i].mshrs[j].entires->name = strdup(buff);

			mshr_clear(&(l1_i_caches[i].mshrs[j]));
		}*/

/*else if (access_type == cgm_access_retry)
			{
				cpu_cache_access_retry(&(l1_d_caches[my_pid]), message_packet);
			}*/

/*else if (access_type == cgm_access_store)
			{
				cache_get_block_status(&(l1_d_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

				printf("cache_block_hit_ptr %d\n", *cache_block_hit_ptr);
				printf("cache_block_state_ptr %d", *cache_block_state_ptr);
				printf("tag %d set %d offset %d", message_packet->tag, message_packet->set, message_packet->offset);
				getchar();

				//hit
				if(*cache_block_hit_ptr && *cache_block_state_ptr != cache_block_invalid)
				{
					//stats;
					l1_d_caches[my_pid].hits++;

					switch(*cache_block_state_ptr)
					{
						case cache_block_invalid:
						case cache_block_noncoherent:
						case cache_block_modified:
						case cache_block_owned:
						case cache_block_exclusive:
							fatal("l1_d_cache_ctrl(): Store, invalid block state on hit\n");
							break;

						case cache_block_shared:
							printf("here cycle %llu\n", P_TIME);
							cache_l1_d_return(&(l1_d_caches[my_pid]),message_packet);
							break;
					}
				}
				//Miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cache_block_invalid)
				{
					l1_d_caches[my_pid].misses++;

					switch(*cache_block_state_ptr)
					{
						case cache_block_noncoherent:
						case cache_block_modified:
						case cache_block_owned:
						case cache_block_exclusive:
							fatal("l1_i_cache_ctrl(): Store, invalid block state on miss\n");
							break;

						case cache_block_invalid:
						case cache_block_shared:


							break;
					}
				}
			}*/
			///////////protocol v2

			/*if (access_type == cgm_access_load)
			{
				cpu_l1_cache_access_load(&(l1_d_caches[my_pid]), message_packet);
			}*/

			/*else if (access_type == cgm_access_store)
			{
				cpu_l1_cache_access_store(&(l1_d_caches[my_pid]), message_packet);
			}*/

/*printf("access_id %llu l2 hit as %s cycle %llu\n", message_packet->access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
				getchar();*/

/*printf("running here\n");*/

/*printf("access_id %llu l2 as %s cycle %llu\n", message_packet->access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
			getchar();*/

/*printf("access_id %llu l2 hit as %s cycle %llu\n", message_packet->access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
							getchar();*/

/*printf("access_id %llu l2 coal as %s cycle %llu\n", access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
								getchar();*/

/*printf("access_id %llu l2 as %s cycle %llu\n", access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);*/

							/*printf("access_id %llu l2 miss as %s cycle %llu\n", access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
							getchar();*/

/*printf("tag %d set %d offset %d", message_packet->tag, message_packet->set, message_packet->offset);
							getchar();*/

///////////protocol v2

			/*if (access_type == cgm_access_fetch)
			{
				cpu_l1_cache_access_load(&(l1_i_caches[my_pid]), message_packet);
			}*/



//block is returned so find it in the ORT
	/*for (i = 0; i < cache->mshr_size; i++)
	{
		if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
		{
			//hit in the ORT table
			break;
		}
	}

	if(i >= cache->mshr_size)
	{
		//if we didn't find it there is a problem;
		printf("gpu_cache_access_put() crashing %s access_id %llu cycle %llu\n", cache->name, access_id, P_TIME);
		printf("src %s dest %s\n", message_packet->src_name, message_packet->dest_name);
		fflush(stdout);
		assert(i < cache->mshr_size);
		assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);
	}

	//clear the ORT now
	cache->ort[i][0] = -1;
	cache->ort[i][1] = -1;
	cache->ort[i][2] = -1;*/

//miss so check ORT status
			/*for (i = 0; i <  cache->mshr_size; i++)
			{
				if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
				{
					//hit in the ORT table
					break;
				}
			}*/


			/*if(i == cache->mshr_size)
			{*/
				//get an empty row
				/*for (i = 0; i <  cache->mshr_size; i++)
				{
					if(cache->ort[i][0] == -1 && cache->ort[i][1] == -1 && cache->ort[i][2] == -1)
					{
						//found empty row
						break;
					}
				}

				//sanity check the table row
				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);

				//insert into table
				cache->ort[i][0] = tag;
				cache->ort[i][1] = set;
				cache->ort[i][2] = 1;*/

/*}
			else if (i >= 0 && i < cache->mshr_size)
			{
				//entry found in ORT so coalesce access
				assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);

				CGM_DEBUG(GPU_cache_debug_file, "%s access_id %llu cycle %llu coalesced\n",
						cache->name, access_id, P_TIME);

				list_remove(cache->last_queue, message_packet);
				list_enqueue(cache->ort_list, message_packet);
			}
			else
			{
				fatal("gpu_cache_access_get(): %s ort row outside of bounds\n", cache->name);
			}*/

/*//get an empty row
			for (i = 0; i <  cache->mshr_size; i++)
			{
				if(cache->ort[i][0] == -1 && cache->ort[i][1] == -1 && cache->ort[i][2] == -1)
				{
					//found empty row
					break;
				}
			}

			//sanity check the table row
			//printf("i = %d\n", i);
			if(i >= cache->mshr_size)
			{

				fatal("gpu_l1_cache_access_store(): mshr full access_id %llu cycle %llu", access_id, P_TIME);
				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);
			}

			//insert into table
			cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

			/*while(!cache_can_access_top(&gpu_l2_caches[cgm_gpu_cache_map(cache->id)]))
			{
				printf("%s stalled cycle %llu \n", cache->name, P_TIME);
				P_PAUSE(1);
			}*/

for (i = 0; i <  cache->mshr_size; i++)
		{
			if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
			{
				//hit in the ORT table
				break;
			}
		}

//miss so check ORT status
		/*for (i = 0; i <  cache->mshr_size; i++)
		{
			if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
			{
				//hit in the ORT table
				break;
			}
		}*/
		
		
		/*for (i = 0; i <  cache->mshr_size; i++)
			{
				if(cache->ort[i][0] == -1 && cache->ort[i][1] == -1 && cache->ort[i][2] == -1)
				{
					//found empty row
					break;
				}
			}

			//sanity check the table row
			assert(i < cache->mshr_size);
			assert(cache->ort[i][0] == -1);
			assert(cache->ort[i][1] == -1);
			assert(cache->ort[i][2] == -1);

			//insert into table
			cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

			//forward message_packet
			/*while(!cache_can_access_top(&gpu_l2_caches[cgm_gpu_cache_map(cache->id)]))
			{
				printf("%s stalled cycle %llu \n", cache->name, P_TIME);

				P_PAUSE(1);
			}*/


//future_advance(&switches_ec[cache->id], WIRE_DELAY(switches[cache->id].wire_latency));

			/*}
			else if (i >= 0 && i < cache->mshr_size)
			{

				printf("%s entered here coal cycle %llu\n", cache->name, P_TIME);
				fflush(stdout);


				//entry found in ORT so coalesce access
				assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu coalesced\n",
						cache->name, access_id, P_TIME);

				list_remove(cache->last_queue, message_packet);
				list_enqueue(cache->ort_list, message_packet);
			}
			else
			{

				fatal("cpu_cache_access_get): %s i outside of bounds\n", cache->name);
			}*/

			//sanity check the table row
			/*if(i >= cache->mshr_size)
			{
				printf("%s crashing ort is full\n", cache->name);

				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);
			}

			//insert into table
			cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

			/*while(!switch_can_access(switches[cache->id].south_queue))
			{
				//printf("stall\n");
				P_PAUSE(1);
			}*/


//sanity check the table row
				/*if(i >= cache->mshr_size)
				{
					printf("%s crashing ort is full here i = %d\n", cache->name, i);
					ort_dump(cache);
					assert(i < cache->mshr_size);
					assert(cache->ort[i][0] == -1);
					assert(cache->ort[i][1] == -1);
					assert(cache->ort[i][2] == -1);
				}

				//insert into table
				cache->ort[i][0] = tag;
				cache->ort[i][1] = set;
				cache->ort[i][2] = 1;*/

				/*while(!switch_can_access(switches[cache->id].north_queue))
				{
					//printf("stall\n");
					P_PAUSE(1);
				}*/


/*if(i >= cache->mshr_size)
	{
		//if we didn't find it there is a problem;
		printf("cpu_cache_access_put() crashing %s access_id %llu cycle %llu\n", cache->name, access_id, P_TIME);
		printf("src %s dest %s\n", message_packet->src_name, message_packet->dest_name);
		fflush(stdout);
		assert(i < cache->mshr_size);
		assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);
	}*/

//i = get_ort_status(cache);

			//sanity check the table row
			/*if(i >= cache->mshr_size)
			{
				advance(cache->ec_ptr);
				return;
				printf ("i = %d\n", i);
				printf("%s crashing ort is full access_id %llu cycle %llu\n", cache->name, access_id, P_TIME);
				ort_dump(cache);
				STOP;
				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);
			}*/


			//insert into table
			/*cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

/*for (i = 0; i <  cache->mshr_size; i++)
		{
			if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
			{
				//hit in the ORT table
				break;
			}
		}*/

/*for (i = 0; i <  cache->mshr_size; i++)
			{
				if(cache->ort[i][0] == -1 && cache->ort[i][1] == -1 && cache->ort[i][2] == -1)
				{
					//found empty row
					break;
				}
			}

			//sanity check the table row
			if(i >= cache->mshr_size)
			{
				printf("%s crashing ort is full access_id %llu cycle %llu\n", cache->name, access_id, P_TIME);
				ort_dump(cache);
				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);
			}


			//insert into table
			cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

			/*while(!cache_can_access_top(&l2_caches[cache->id]))
			{
				//printf("stall\n");
				P_PAUSE(1);
			}*/

if()
				{
				}
				else
				{
					printf("%s stalling cycle %llu\n", gpu_l2_caches[my_pid].name, P_TIME);
					future_advance(&gpu_l2_cache[my_pid], etime.count + 2);
				}


				if(cache_can_access_top(&gpu_l2_caches[cgm_gpu_cache_map(my_pid)]))
				{
				}
				else
				{
					printf("%s stalling cycle %llu\n", gpu_v_caches[my_pid].name, P_TIME);
					future_advance(&gpu_v_cache[my_pid], etime.count + 2);
				}

if()
				{
				}
				else
				{
					printf("%s stalling cycle %llu\n", gpu_v_caches[my_pid].name, P_TIME);
					future_advance(&gpu_v_cache[my_pid], etime.count + 2);
				}

	if()
				{
				}
				else
				{
					printf("stalling\n");
					future_advance(&gpu_s_cache[my_pid], etime.count + 2);
				}


/*if()
				{
				}
				else
				{
					//we have to wait because the L2 in queue is full
					//printf("running here\n");
					future_advance(&l3_cache[my_pid], etime.count + 2);
				}*/

	
	/*if()
				{
				}
				else
				{
					//we have to wait because the switch in queue is full
					PRINT("l1_d_cache can't run load cycle %llu\n", P_TIME);
					step--;
					P_PAUSE(1);
					//future_advance(&l2_cache[my_pid], etime.count + 2);
				}*/
	
	/*if(cache_can_access_top(&l2_caches[my_pid]))
				{*/
				/*}
				else
				{
					//we have to wait because the L2 in queue is full
					PRINT("l1_d_cache can't run store cycle %llu\n", P_TIME);
					step--;
					P_PAUSE(1);
					//future_advance(&l1_d_cache[my_pid], etime.count + 2);
				}*/
	
	
		/*message is from the CPU, only fetch
				if the in queue of the L2 cache has an open slot*/
				/*if(cache_can_access_top(&l2_caches[my_pid]))
				{*/
				/*}
				else
				{
					//we have to wait because the L2 in queue is full
					PRINT("l1_d_cache can't run load cycle %llu\n", P_TIME);
					step--;
					P_PAUSE(1);
					//future_advance(&l1_d_cache[my_pid], etime.count + 2);
				}*/
	
	
	//PRINT("l1_d_cache null packet cycle %llu\n", P_TIME);
			//PRINT("l1_d_cache stall this cycle %llu\n", P_TIME);
			
	
	
		/*message is from the CPU, only fetch
				if the in queue of the L2 cache has an open slot*/
				/*if(cache_can_access_top(&l2_caches[my_pid]))
				{*/
				/*}
				else
				{
					//we have to wait because the L2 in queue is full
					PRINT("l1_i_cache can't run load cycle %llu\n", P_TIME);
					step--;
					P_PAUSE(1);
					//future_advance(&l1_i_cache[my_pid], etime.count + 2);
				}*/
	
	
	/*void gpu_l2_cache_access_load(struct cache_t *cache, struct cgm_packet_t *message_packet){

	else if (access_type == cgm_access_load)
			{
				//stats
				l2_caches[my_pid].loads++;

				cache_status = cgm_cache_find_block(&(l2_caches[my_pid]), tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

				// L2 Cache Hit!
				if(cache_status == 1)
				{
					//stats
					l2_caches[my_pid].hits++;

					//This is a hit in the L2 cache need to send up to L1 cache
					//remove packet from l2 cache in queue
					message_packet->access_type = cgm_access_l2_load_reply;

					list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
					list_enqueue(l1_d_caches[my_pid].Rx_queue_top, message_packet);
					//cgm_cache_set_block(&(l2_caches[0]), *set_ptr, *way_ptr, tag, 1);

					future_advance(l1_d_cache, (etime.count + l1_d_caches[my_pid].wire_latency));
				}
				// L2 Cache Miss!
				else if(cache_status == 0)
				{
					//stats
					l2_caches[my_pid].misses++;
					//for now pretend that it is the last level of cache and memory ctrl.
					P_PAUSE(mem_miss);

					message_packet->access_type = cgm_access_l2_load_reply;

					cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, 4);

					list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
					list_enqueue(l1_d_caches[my_pid].Rx_queue_top, message_packet);

					future_advance(l1_d_cache, (etime.count + l1_d_caches[my_pid].wire_latency));
				}

			}
	return;
}*/

/*void gpu_l2_cache_access_store(struct cache_t *cache, struct cgm_packet_t *message_packet){

	else if (access_type == cgm_access_store)
	{
		//stats
		l2_caches[my_pid].stores++;
		cache_status = cgm_cache_find_block(&(l2_caches[my_pid]), tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

		// L2 Cache Hit!
		if(cache_status == 1)
		{
			//stats
			l2_caches[my_pid].hits++;

			cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, 4);
					//This is a hit in the L2 cache need to send up to L1 cache
					//remove packet from l2 cache in queue
					message_packet->access_type = cgm_access_l2_store_reply;
					list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
					list_enqueue(l1_d_caches[my_pid].Rx_queue_top, message_packet);
					//cgm_cache_set_block(&(l2_caches[0]), *set_ptr, *way_ptr, tag, 1);

					future_advance(l1_d_cache, (etime.count + l1_d_caches[my_pid].wire_latency));
				}
				// L2 Cache Miss!
				else if(cache_status == 0)
				{
					//stats
					l2_caches[my_pid].misses++;

					//for now pretend that it is the last level of cache and memory ctrl.
					P_PAUSE(mem_miss);

					message_packet->access_type = cgm_access_l2_store_reply;

					cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, 4);

					list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
					list_enqueue(l1_d_caches[my_pid].Rx_queue_top, message_packet);

					future_advance(l1_d_cache, (etime.count + l1_d_caches[my_pid].wire_latency));
				}
			}
		//}

	return;
}*/
	
	
	
	
	
/*old code
		//while the next level of cache's in queue is full stall
		while(!cache_can_access_bottom(&l1_i_caches[cache->id]))
		{
			P_PAUSE(1);
		}

		//change access type, i cache only ever reads so puts is ok.
		message_packet->access_type = cgm_access_puts;

		//success, remove packet from l2 cache in queue
		list_remove(cache->last_queue, message_packet);

		CGM_DEBUG(GPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
				cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

		list_enqueue(l1_i_caches[cache->id].Rx_queue_bottom, message_packet);
		future_advance(&l1_i_cache[cache->id], WIRE_DELAY(l1_i_caches[cache->id].wire_latency));
	}
	else
	{
		fatal("l2_cache_access_retry(): miss on retry\n");
	}*/

/*void cpu_l1_cache_access_puts(struct cache_t *cache, struct cgm_packet_t *message_packet){

	struct cgm_packet_t *miss_status_packet;
	enum cgm_access_kind_t access_type;
	unsigned int addr = 0;
	long long access_id = 0;
	int set = 0;
	int tag = 0;
	unsigned int offset = 0;
	int way = 0;
	int state = 0;

	int *set_ptr = &set;
	int *tag_ptr = &tag;
	unsigned int *offset_ptr = &offset;
	int *way_ptr = &way;
	int *state_ptr = &state;

	int mshr_row = -1;

	int i = 0;

	//the packet is from the L2 cache
	access_type = message_packet->access_type;
	addr = message_packet->address;
	access_id = message_packet->access_id;

	//probe the address for set, tag, and offset.
	assert(addr != NULL);
	cgm_cache_decode_address(cache, addr, set_ptr, tag_ptr, offset_ptr);

	CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu puts\n", cache->name, access_id, P_TIME);

	//charge the delay for writing cache block
	cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
	P_PAUSE(cache->latency);

	//get the mshr status
	mshr_row = mshr_get_status(cache, set_ptr, tag_ptr, access_id);
	if(mshr_row == -1)
	{
		printf("%s crashing %llu access_id %llu type %s\n", cache->name, P_TIME, access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->cpu_access_type));
		mshr_dump(cache);
		assert(mshr_row != -1);
	}

	//check the number of entries in the mshr row
	assert(list_count(cache->mshrs[mshr_row].entires) == cache->mshrs[mshr_row].num_entries);
	assert(cache->mshrs[mshr_row].num_entries > 0);

	CGM_DEBUG(mshr_debug_file, "%s access_id %llu cycle %llu mshr_row %d num_entries %d\n", cache->name, access_id, P_TIME, mshr_row, cache->mshrs[mshr_row].num_entries);

	//move them to the retry queue
	for(i = 0; i < cache->mshrs[mshr_row].num_entries; i++)
	{
		miss_status_packet = list_dequeue(cache->mshrs[mshr_row].entires);

		CGM_DEBUG(mshr_debug_file, "%s access_id %llu coalesced %d tag %d set %d\n",
				cache->name, miss_status_packet->access_id, miss_status_packet->coalesced, miss_status_packet->tag, miss_status_packet->set);

		assert(miss_status_packet != NULL);
		//assert(miss_status_packet->address != 0);

		if (miss_status_packet->access_id == access_id)
		{
			//this is the first entry and was not coalesced
			//assert(miss_status_packet->access_id == access_id);
			assert(miss_status_packet->coalesced == 0);

			//we can put either the message_packet or miss_status_packet in the retry queue.
			message_packet->access_type = cgm_access_retry;
			list_remove(cache->last_queue, message_packet);
			list_enqueue(cache->retry_queue, message_packet);

		}
		else
		{
			//this is a coalesced packet
			if(miss_status_packet->coalesced != 1)
			{
				printf("breaking access_id %llu cycle %llu\n", access_id, P_TIME);
				printf("i %d miss sp %llu, coalesced %d\n", i, miss_status_packet->access_id, miss_status_packet->coalesced);

				mshr_dump(cache);

				STOP;
			}

			assert(miss_status_packet->coalesced == 1);

			//drop it into the retry queue
			list_enqueue(cache->retry_queue, miss_status_packet);

		}
	}

	//advance myself by the number of packets.
	long long time = etime.count;  :-P
	for(i = 0; i < cache->mshrs[mshr_row].num_entries; i ++)
	{
		time += 2;
		future_advance(cache->ec_ptr, time);
	}

	//clear the mshr row for future use
	mshr_clear(&(cache->mshrs[mshr_row]));

	return;
}*/


//struct cgm_packet_status_t *mshr_remove(struct cache_t *cache);



/*struct cgm_packet_status_t *mshr_remove(struct cache_t *cache){

	int i = 0;
	struct cgm_packet_status_t *miss_status_packet;
	struct cgm_packet_status_t *message_packet;


	list_enqueue(cache->retry_queue, message_packet);



	LIST_FOR_EACH(cache->mshr, i)
	{
		mshr_packet = list_get(cache->mshr, i);

		if (mshr_packet->access_id == access_id)
		{
			return list_remove_at(cache->mshr, i);
		}
	}

	return NULL;
}*/



//this needs to be deleted
//int cache_get_state(struct cache_t *cache, enum cgm_access_kind_t access_type, int *tag_ptr, int *set_ptr, unsigned int *offset_ptr, int *way_ptr, int *state_ptr);


//void cpu_l1_cache_access_puts(struct cache_t *cache, struct cgm_packet_t *message_packet);

/*int cache_get_state(struct cache_t *cache, enum cgm_access_kind_t access_type, int *tag_ptr, int *set_ptr, unsigned int *offset_ptr, int *way_ptr, int *state_ptr){

	cache_block_invalid = 0
	cache_block_noncoherent = 1
	cache_block_modified = 2
	cache_block_owned = 3
	cache_block_exclusive = 4
	cache_block_shared = 5

	int cache_status;

	//stats


	//find the block in the cache and get it's state
	cache_status = cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

	printf("cache_status %d\n", cache_status);
	getchar();

	//hit and state is M, E, or S we are done at this level of cache
	if((cache_status == 1 && *state_ptr == 2) || (cache_status == 1 && *state_ptr == 4 ) || (cache_status == 1 && *state_ptr == 5))
	{
		//stats
		cache->hits++;

		//done, respond to requester.
		return 1;
	}
	//hit and state is invalid (miss)
	else if(cache_status == 1 && *state_ptr == 0)
	{
		//stats
		cache->invalid_hits++;

		//treat this like a miss
		return 2;

	}
	//the cache block is not present m the cache (miss)
	else if(cache_status == 0)
	{
		//stats
		cache->misses++;

		return 3;
	}
	else if (cache_status == 1 && *state_ptr == 1)
	{
		printf("CRASHING cache_status %d state_ptr %d\n", cache_status, *state_ptr);
		getchar();
		fatal("cache_mesi_load() non cached state\n");
	}
	else
	{
		printf("CRASHING cache_status %d state_ptr %d\n", cache_status, *state_ptr);
		getchar();
		fatal("cache_mesi_load() something went wrong here\n");
	}

	return 0;

}*/


//fixed
	//===============================================
	/*if(mshr_status >= 0)
	{
		we have outstanding mshr requests so set the retry state bit
		*retry_ptr = cache->mshrs[mshr_status].num_entries;
		assert(*retry_ptr > 0);
	}

	//move the access and any coalesced accesses to the retry queue.
	for(i = 0; i < *retry_ptr; i++)
	{
		if( i == 0)
		{
			//move current message_packet to retry queue
			message_packet->access_type = cgm_access_retry;
			list_remove(cache->last_queue, message_packet);
			list_enqueue(cache->retry_queue, message_packet);
			advance(&l2_cache[cache->id]);
		}
		else if( i > 0)
		{
			miss_status_packet = list_remove_at(cache->mshrs[mshr_status].entires, i);
			miss_status_packet->coalesced_packet->access_type = cgm_access_retry;
			list_enqueue(cache->retry_queue, miss_status_packet->coalesced_packet);
			free(miss_status_packet);
			advance(&l2_cache[cache->id]);
		}
	}

	mshr_clear(&(cache->mshrs[mshr_status]));*/










//fixed
		/*if(mshr_status == 1)
		{
			//access is unique in the MSHR
			//while the next level's queue is full stall
			while(!switch_can_access(switches[cache->id].north_queue))
			{
				P_PAUSE(1);
			}

			CGM_DEBUG(cache_debug_file, "l2_cache[%d] access_id %llu cycle %llu miss switch north queue free size %d\n",
					cache->id, access_id, P_TIME, list_count(switches[cache->id].north_queue));

			//send to L3 cache over switching network add source and dest here
			//star todo send to correct l3 dest
			message_packet->access_type = cgm_access_gets_i;
			message_packet->src_name = cache->name;
			message_packet->source_id = str_map_string(&node_strn_map, cache->name);
			message_packet->dest_name = l3_caches[cache->id].name;
			message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[cache->id].name);


			list_remove(cache->last_queue, message_packet);
			CGM_DEBUG(cache_debug_file, "l2_cache[%d] access_id %llu cycle %llu removed from %s size %d\n",
					cache->id, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));
			list_enqueue(switches[cache->id].north_queue, message_packet);

			future_advance(&switches_ec[cache->id], WIRE_DELAY(switches[cache->id].wire_latency));

			CGM_DEBUG(cache_debug_file, "l2_cache[%d] access_id %llu cycle %llu l2_cache[%d] as %s\n",
				cache->id, access_id, P_TIME, cache->id, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

			CGM_DEBUG(protocol_debug_file, "Access_id %llu cycle %llu l1_i_cache[%d] Miss\tSEND l2_cache[%d] -> %s\n",
				access_id, P_TIME, cache->id, cache->id, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

		}*/


//==================================


			/*//stats
			if(access_type == cgm_access_fetch)
				l1_i_caches[my_pid].fetches++;

			if(access_type == cgm_access_retry)
				l1_i_caches[my_pid].retries++;

			//memory access from CPU
			addr = message_packet->address;
			access_id = message_packet->access_id;

			//probe the address for set, tag, and offset.
			cgm_cache_decode_address(&(l1_i_caches[my_pid]), addr, set_ptr, tag_ptr, offset_ptr);

			CGM_DEBUG(cache_debug_file,"l1_i_cache[%d] access_id %llu cycle %llu as %s addr 0x%08u, tag %d, set %d, offset %u\n",
					my_pid, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, access_type), addr, *tag_ptr, *set_ptr, *offset_ptr);

			//get the block and the state of the block and charge a cycle
			cache_status = cgm_cache_find_block(&(l1_i_caches[my_pid]), tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
			P_PAUSE(2);

			//L1 I Cache Hit!
			if(cache_status == 1 && *state_ptr != 0)
			{
				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu hit\n", my_pid, access_id, P_TIME);

				if(access_type == cgm_access_retry)
					retry_ptr--;

				if(access_type == cgm_access_fetch)
					l1_i_caches[my_pid].hits++;

				//remove packet from cache queue, global queue, and simulator memory
				//note cycle already charged

				list_remove(l1_i_caches[my_pid].last_queue, message_packet);
				remove_from_global(access_id);
				free(message_packet);

			}
			//L1 I Cache Miss!
			else if(cache_status == 0 || *state_ptr == 0)
			{
				//all mshr based retires should be hits
				//star todo there is a bug here 1 access fails retry in our MM.
				assert(message_packet->access_type != cgm_access_retry);

				if(access_type == cgm_access_fetch)
					l1_i_caches[my_pid].misses++;

				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss\n", my_pid, access_id, P_TIME);

				miss_status_packet = miss_status_packet_create(message_packet->access_id, message_packet->access_type, set, tag, offset);
				mshr_status = mshr_set(&(l1_i_caches[my_pid]), miss_status_packet, message_packet);

				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss mshr status %d\n", my_pid, access_id, P_TIME, mshr_status);

				if(mshr_status == 1)
				{
					//access is unique in the MSHR
					//while the next level of cache's in queue is full stall
					while(!cache_can_access_top(&l2_caches[my_pid]))
					{
						P_PAUSE(1);
					}

					CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss l2 queue free\n", my_pid, access_id, P_TIME);

					change the access type for the coherence protocol and drop into the L2's queue
					remove the access from the l1 cache queue and place it in the l2 cache ctrl queue
					message_packet->access_type = cgm_access_gets_i;
					list_remove(l1_i_caches[my_pid].last_queue, message_packet);
					list_enqueue(l2_caches[my_pid].Rx_queue_top, message_packet);

					CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu l2_cache[%d] -> %s\n",
							my_pid, access_id, P_TIME, my_pid, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

					//advance the L2 cache adding some wire delay time.
					future_advance(&l2_cache[my_pid], (etime.count + (l2_caches[my_pid].wire_latency * 2)));
				}
				else if(mshr_status == 0)
				{
					//mshr is full so we can't progress, retry.
					message_packet->access_type = cgm_access_retry;
					future_advance(&l1_i_cache[my_pid], (etime.count + 2));

				}
				else
				{
					//access was coalesced. For now do nothing until later.
				}

				//done
			}
		}*/

		/*else if(access_type == cgm_access_puts)
		{
			//the packet is from the L2 cache
			addr = message_packet->address;
			access_id = message_packet->access_id;

			//probe the address for set, tag, and offset.
			cgm_cache_decode_address(&(l1_i_caches[my_pid]), addr, set_ptr, tag_ptr, offset_ptr);

			CGM_DEBUG(cache_debug_file, "l1_i_cache[%d] access_id %llu cycle %llu puts\n", my_pid, access_id, P_TIME);

			//charge the delay for writing cache block
			cgm_cache_set_block(&l1_i_caches[my_pid], *set_ptr, *way_ptr, tag, cache_block_shared);
			P_PAUSE(1);

			//get the mshr status
			mshr_status = mshr_get(&l1_i_caches[my_pid], set_ptr, tag_ptr);
			assert(mshr_status != -1);

			if(mshr_status >= 0)
			{
				we have outstanding mshr requests so set the retry state bit
				*retry_ptr = l1_i_caches[my_pid].mshrs[mshr_status].num_entries;
				//printf("retry_ptr %d\n", *retry_ptr);
				assert(*retry_ptr > 0);
			}

			advance_time = etime.count + 2;

			//move the access and any coalesced accesses to the retry queue.
			for(i = 0; i < *retry_ptr; i++)
			{
				if( i == 0)
				{
					//move current message_packet to retry queue
					message_packet->access_type = cgm_access_retry;
					list_remove(l1_i_caches[my_pid].next_queue, message_packet);
					list_enqueue(l1_i_caches[my_pid].retry_queue, message_packet);

					//printf("list count %d\n", list_count(l1_i_caches[my_pid].retry_queue));

					advance(&l1_i_cache[my_pid]);
				}
				else if( i > 0)
				{
					miss_status_packet = list_remove_at(l1_i_caches[my_pid].mshrs[mshr_status].entires, i);
					list_enqueue(l1_i_caches[my_pid].retry_queue, miss_status_packet->coalesced_packet);
					free(miss_status_packet);
					advance_time += 2;
					advance(&l1_i_cache[my_pid]);
				}
			}

			//clear the mshr row for future use
			mshr_clear(&l1_i_caches[my_pid].mshrs[mshr_status]);
			//done.
		}


	}*/


else if(cache_status == 0 || *state_ptr == 0)
{

				// L2 Cache Miss!
				l2_caches[my_pid].misses++;

				/*printf("access id %llu l1 miss\n", access_id);
				getchar();*/


				//star todo check on size of MSHR
				mshr_packet = status_packet_create();

				//drop a token in the mshr queue
				//star todo add some detail to this so we can include coalescing
				//star todo have an MSHR hit advance the cache and clear out the request.
				mshr_packet->access_type = message_packet->access_type;
				mshr_packet->access_id = message_packet->access_id;
				mshr_packet->in_flight = message_packet->in_flight;
				list_enqueue(l2_caches[my_pid].mshr, mshr_packet);


				message_packet->access_type = cgm_access_puts;

				//set the block now for testing///////////
				cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, cache_block_shared);
				///////////////////////////////////////////

				list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
				list_enqueue(l1_i_caches[my_pid].Rx_queue_top, message_packet);

				future_advance(&l1_i_cache[my_pid], (etime.count + l1_i_caches[my_pid].wire_latency));
			}
			
			
			
//printf("After probe addr 0x%08x\n", addr);

	/*printf("cache->log_block_size = %d\n",cache->log_block_size);
	printf("cache->block_mask %d\n", cache->block_mask);
	printf("\n");*/
	
	//notes this is useing the tag and indx to calculate set location.

	/*printf("---set_ptr---\n");
	printf("Addr 0x%08x\n", addr);
	printf("(addr >> cache->log_block_size) = 0x%08x\n", addr >> cache->log_block_size);
	printf("set_ptr %d\n", (addr >> cache->log_block_size) % cache->num_sets);
	printf("---set_ptr---\n");*/

	/*printf("---tag_ptr---\n");
	printf("Addr 0x%08X\n", addr);
	printf("~(cache->block_mask) 0x%08x\n", ~(cache->block_mask));
	printf("addr & ~(cache->block_mask) 0x%08x\n", addr & ~(cache->block_mask));
	printf("tag %d\n", *tag_ptr);
	printf("---tag_ptr---\n");
	getchar();*/

	/*printf("---offset_ptr---\n");
	printf("Addr 0x%08x\n", addr);
	printf("(cache->block_mask) 0x%08x\n", (cache->block_mask));
	printf("addr & (cache->block_mask) 0x%08x\n", addr & addr & (cache->block_mask));
	printf("---offset_ptr---\n");
	getchar();*/
	
	