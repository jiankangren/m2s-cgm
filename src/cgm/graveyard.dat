



/*else if (access_type == cgm_access_retry)
			{
				cpu_cache_access_retry(&(l1_d_caches[my_pid]), message_packet);
			}*/

/*else if (access_type == cgm_access_store)
			{
				cache_get_block_status(&(l1_d_caches[my_pid]), message_packet, cache_block_hit_ptr, cache_block_state_ptr);

				printf("cache_block_hit_ptr %d\n", *cache_block_hit_ptr);
				printf("cache_block_state_ptr %d", *cache_block_state_ptr);
				printf("tag %d set %d offset %d", message_packet->tag, message_packet->set, message_packet->offset);
				getchar();

				//hit
				if(*cache_block_hit_ptr && *cache_block_state_ptr != cache_block_invalid)
				{
					//stats;
					l1_d_caches[my_pid].hits++;

					switch(*cache_block_state_ptr)
					{
						case cache_block_invalid:
						case cache_block_noncoherent:
						case cache_block_modified:
						case cache_block_owned:
						case cache_block_exclusive:
							fatal("l1_d_cache_ctrl(): Store, invalid block state on hit\n");
							break;

						case cache_block_shared:
							printf("here cycle %llu\n", P_TIME);
							cache_l1_d_return(&(l1_d_caches[my_pid]),message_packet);
							break;
					}
				}
				//Miss
				else if(*cache_block_hit_ptr || *cache_block_state_ptr == cache_block_invalid)
				{
					l1_d_caches[my_pid].misses++;

					switch(*cache_block_state_ptr)
					{
						case cache_block_noncoherent:
						case cache_block_modified:
						case cache_block_owned:
						case cache_block_exclusive:
							fatal("l1_i_cache_ctrl(): Store, invalid block state on miss\n");
							break;

						case cache_block_invalid:
						case cache_block_shared:


							break;
					}
				}
			}*/
			///////////protocol v2

			/*if (access_type == cgm_access_load)
			{
				cpu_l1_cache_access_load(&(l1_d_caches[my_pid]), message_packet);
			}*/

			/*else if (access_type == cgm_access_store)
			{
				cpu_l1_cache_access_store(&(l1_d_caches[my_pid]), message_packet);
			}*/

/*printf("access_id %llu l2 hit as %s cycle %llu\n", message_packet->access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
				getchar();*/

/*printf("running here\n");*/

/*printf("access_id %llu l2 as %s cycle %llu\n", message_packet->access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
			getchar();*/

/*printf("access_id %llu l2 hit as %s cycle %llu\n", message_packet->access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
							getchar();*/

/*printf("access_id %llu l2 coal as %s cycle %llu\n", access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
								getchar();*/

/*printf("access_id %llu l2 as %s cycle %llu\n", access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);*/

							/*printf("access_id %llu l2 miss as %s cycle %llu\n", access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->access_type), P_TIME);
							getchar();*/

/*printf("tag %d set %d offset %d", message_packet->tag, message_packet->set, message_packet->offset);
							getchar();*/

///////////protocol v2

			/*if (access_type == cgm_access_fetch)
			{
				cpu_l1_cache_access_load(&(l1_i_caches[my_pid]), message_packet);
			}*/



//block is returned so find it in the ORT
	/*for (i = 0; i < cache->mshr_size; i++)
	{
		if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
		{
			//hit in the ORT table
			break;
		}
	}

	if(i >= cache->mshr_size)
	{
		//if we didn't find it there is a problem;
		printf("gpu_cache_access_put() crashing %s access_id %llu cycle %llu\n", cache->name, access_id, P_TIME);
		printf("src %s dest %s\n", message_packet->src_name, message_packet->dest_name);
		fflush(stdout);
		assert(i < cache->mshr_size);
		assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);
	}

	//clear the ORT now
	cache->ort[i][0] = -1;
	cache->ort[i][1] = -1;
	cache->ort[i][2] = -1;*/

//miss so check ORT status
			/*for (i = 0; i <  cache->mshr_size; i++)
			{
				if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
				{
					//hit in the ORT table
					break;
				}
			}*/


			/*if(i == cache->mshr_size)
			{*/
				//get an empty row
				/*for (i = 0; i <  cache->mshr_size; i++)
				{
					if(cache->ort[i][0] == -1 && cache->ort[i][1] == -1 && cache->ort[i][2] == -1)
					{
						//found empty row
						break;
					}
				}

				//sanity check the table row
				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);

				//insert into table
				cache->ort[i][0] = tag;
				cache->ort[i][1] = set;
				cache->ort[i][2] = 1;*/

/*}
			else if (i >= 0 && i < cache->mshr_size)
			{
				//entry found in ORT so coalesce access
				assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);

				CGM_DEBUG(GPU_cache_debug_file, "%s access_id %llu cycle %llu coalesced\n",
						cache->name, access_id, P_TIME);

				list_remove(cache->last_queue, message_packet);
				list_enqueue(cache->ort_list, message_packet);
			}
			else
			{
				fatal("gpu_cache_access_get(): %s ort row outside of bounds\n", cache->name);
			}*/

/*//get an empty row
			for (i = 0; i <  cache->mshr_size; i++)
			{
				if(cache->ort[i][0] == -1 && cache->ort[i][1] == -1 && cache->ort[i][2] == -1)
				{
					//found empty row
					break;
				}
			}

			//sanity check the table row
			//printf("i = %d\n", i);
			if(i >= cache->mshr_size)
			{

				fatal("gpu_l1_cache_access_store(): mshr full access_id %llu cycle %llu", access_id, P_TIME);
				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);
			}

			//insert into table
			cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

			/*while(!cache_can_access_top(&gpu_l2_caches[cgm_gpu_cache_map(cache->id)]))
			{
				printf("%s stalled cycle %llu \n", cache->name, P_TIME);
				P_PAUSE(1);
			}*/

for (i = 0; i <  cache->mshr_size; i++)
		{
			if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
			{
				//hit in the ORT table
				break;
			}
		}

//miss so check ORT status
		/*for (i = 0; i <  cache->mshr_size; i++)
		{
			if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
			{
				//hit in the ORT table
				break;
			}
		}*/
		
		
		/*for (i = 0; i <  cache->mshr_size; i++)
			{
				if(cache->ort[i][0] == -1 && cache->ort[i][1] == -1 && cache->ort[i][2] == -1)
				{
					//found empty row
					break;
				}
			}

			//sanity check the table row
			assert(i < cache->mshr_size);
			assert(cache->ort[i][0] == -1);
			assert(cache->ort[i][1] == -1);
			assert(cache->ort[i][2] == -1);

			//insert into table
			cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

			//forward message_packet
			/*while(!cache_can_access_top(&gpu_l2_caches[cgm_gpu_cache_map(cache->id)]))
			{
				printf("%s stalled cycle %llu \n", cache->name, P_TIME);

				P_PAUSE(1);
			}*/


//future_advance(&switches_ec[cache->id], WIRE_DELAY(switches[cache->id].wire_latency));

			/*}
			else if (i >= 0 && i < cache->mshr_size)
			{

				printf("%s entered here coal cycle %llu\n", cache->name, P_TIME);
				fflush(stdout);


				//entry found in ORT so coalesce access
				assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);

				CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu coalesced\n",
						cache->name, access_id, P_TIME);

				list_remove(cache->last_queue, message_packet);
				list_enqueue(cache->ort_list, message_packet);
			}
			else
			{

				fatal("cpu_cache_access_get): %s i outside of bounds\n", cache->name);
			}*/

			//sanity check the table row
			/*if(i >= cache->mshr_size)
			{
				printf("%s crashing ort is full\n", cache->name);

				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);
			}

			//insert into table
			cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

			/*while(!switch_can_access(switches[cache->id].south_queue))
			{
				//printf("stall\n");
				P_PAUSE(1);
			}*/


//sanity check the table row
				/*if(i >= cache->mshr_size)
				{
					printf("%s crashing ort is full here i = %d\n", cache->name, i);
					ort_dump(cache);
					assert(i < cache->mshr_size);
					assert(cache->ort[i][0] == -1);
					assert(cache->ort[i][1] == -1);
					assert(cache->ort[i][2] == -1);
				}

				//insert into table
				cache->ort[i][0] = tag;
				cache->ort[i][1] = set;
				cache->ort[i][2] = 1;*/

				/*while(!switch_can_access(switches[cache->id].north_queue))
				{
					//printf("stall\n");
					P_PAUSE(1);
				}*/


/*if(i >= cache->mshr_size)
	{
		//if we didn't find it there is a problem;
		printf("cpu_cache_access_put() crashing %s access_id %llu cycle %llu\n", cache->name, access_id, P_TIME);
		printf("src %s dest %s\n", message_packet->src_name, message_packet->dest_name);
		fflush(stdout);
		assert(i < cache->mshr_size);
		assert(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1);
	}*/

//i = get_ort_status(cache);

			//sanity check the table row
			/*if(i >= cache->mshr_size)
			{
				advance(cache->ec_ptr);
				return;
				printf ("i = %d\n", i);
				printf("%s crashing ort is full access_id %llu cycle %llu\n", cache->name, access_id, P_TIME);
				ort_dump(cache);
				STOP;
				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);
			}*/


			//insert into table
			/*cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

/*for (i = 0; i <  cache->mshr_size; i++)
		{
			if(cache->ort[i][0] == tag && cache->ort[i][1] == set && cache->ort[i][2] == 1)
			{
				//hit in the ORT table
				break;
			}
		}*/

/*for (i = 0; i <  cache->mshr_size; i++)
			{
				if(cache->ort[i][0] == -1 && cache->ort[i][1] == -1 && cache->ort[i][2] == -1)
				{
					//found empty row
					break;
				}
			}

			//sanity check the table row
			if(i >= cache->mshr_size)
			{
				printf("%s crashing ort is full access_id %llu cycle %llu\n", cache->name, access_id, P_TIME);
				ort_dump(cache);
				assert(i < cache->mshr_size);
				assert(cache->ort[i][0] == -1);
				assert(cache->ort[i][1] == -1);
				assert(cache->ort[i][2] == -1);
			}


			//insert into table
			cache->ort[i][0] = tag;
			cache->ort[i][1] = set;
			cache->ort[i][2] = 1;*/

			/*while(!cache_can_access_top(&l2_caches[cache->id]))
			{
				//printf("stall\n");
				P_PAUSE(1);
			}*/

if()
				{
				}
				else
				{
					printf("%s stalling cycle %llu\n", gpu_l2_caches[my_pid].name, P_TIME);
					future_advance(&gpu_l2_cache[my_pid], etime.count + 2);
				}


				if(cache_can_access_top(&gpu_l2_caches[cgm_gpu_cache_map(my_pid)]))
				{
				}
				else
				{
					printf("%s stalling cycle %llu\n", gpu_v_caches[my_pid].name, P_TIME);
					future_advance(&gpu_v_cache[my_pid], etime.count + 2);
				}

if()
				{
				}
				else
				{
					printf("%s stalling cycle %llu\n", gpu_v_caches[my_pid].name, P_TIME);
					future_advance(&gpu_v_cache[my_pid], etime.count + 2);
				}

	if()
				{
				}
				else
				{
					printf("stalling\n");
					future_advance(&gpu_s_cache[my_pid], etime.count + 2);
				}


/*if()
				{
				}
				else
				{
					//we have to wait because the L2 in queue is full
					//printf("running here\n");
					future_advance(&l3_cache[my_pid], etime.count + 2);
				}*/

	
	/*if()
				{
				}
				else
				{
					//we have to wait because the switch in queue is full
					PRINT("l1_d_cache can't run load cycle %llu\n", P_TIME);
					step--;
					P_PAUSE(1);
					//future_advance(&l2_cache[my_pid], etime.count + 2);
				}*/
	
	/*if(cache_can_access_top(&l2_caches[my_pid]))
				{*/
				/*}
				else
				{
					//we have to wait because the L2 in queue is full
					PRINT("l1_d_cache can't run store cycle %llu\n", P_TIME);
					step--;
					P_PAUSE(1);
					//future_advance(&l1_d_cache[my_pid], etime.count + 2);
				}*/
	
	
		/*message is from the CPU, only fetch
				if the in queue of the L2 cache has an open slot*/
				/*if(cache_can_access_top(&l2_caches[my_pid]))
				{*/
				/*}
				else
				{
					//we have to wait because the L2 in queue is full
					PRINT("l1_d_cache can't run load cycle %llu\n", P_TIME);
					step--;
					P_PAUSE(1);
					//future_advance(&l1_d_cache[my_pid], etime.count + 2);
				}*/
	
	
	//PRINT("l1_d_cache null packet cycle %llu\n", P_TIME);
			//PRINT("l1_d_cache stall this cycle %llu\n", P_TIME);
			
	
	
		/*message is from the CPU, only fetch
				if the in queue of the L2 cache has an open slot*/
				/*if(cache_can_access_top(&l2_caches[my_pid]))
				{*/
				/*}
				else
				{
					//we have to wait because the L2 in queue is full
					PRINT("l1_i_cache can't run load cycle %llu\n", P_TIME);
					step--;
					P_PAUSE(1);
					//future_advance(&l1_i_cache[my_pid], etime.count + 2);
				}*/
	
	
	/*void gpu_l2_cache_access_load(struct cache_t *cache, struct cgm_packet_t *message_packet){

	else if (access_type == cgm_access_load)
			{
				//stats
				l2_caches[my_pid].loads++;

				cache_status = cgm_cache_find_block(&(l2_caches[my_pid]), tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

				// L2 Cache Hit!
				if(cache_status == 1)
				{
					//stats
					l2_caches[my_pid].hits++;

					//This is a hit in the L2 cache need to send up to L1 cache
					//remove packet from l2 cache in queue
					message_packet->access_type = cgm_access_l2_load_reply;

					list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
					list_enqueue(l1_d_caches[my_pid].Rx_queue_top, message_packet);
					//cgm_cache_set_block(&(l2_caches[0]), *set_ptr, *way_ptr, tag, 1);

					future_advance(l1_d_cache, (etime.count + l1_d_caches[my_pid].wire_latency));
				}
				// L2 Cache Miss!
				else if(cache_status == 0)
				{
					//stats
					l2_caches[my_pid].misses++;
					//for now pretend that it is the last level of cache and memory ctrl.
					P_PAUSE(mem_miss);

					message_packet->access_type = cgm_access_l2_load_reply;

					cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, 4);

					list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
					list_enqueue(l1_d_caches[my_pid].Rx_queue_top, message_packet);

					future_advance(l1_d_cache, (etime.count + l1_d_caches[my_pid].wire_latency));
				}

			}
	return;
}*/

/*void gpu_l2_cache_access_store(struct cache_t *cache, struct cgm_packet_t *message_packet){

	else if (access_type == cgm_access_store)
	{
		//stats
		l2_caches[my_pid].stores++;
		cache_status = cgm_cache_find_block(&(l2_caches[my_pid]), tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

		// L2 Cache Hit!
		if(cache_status == 1)
		{
			//stats
			l2_caches[my_pid].hits++;

			cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, 4);
					//This is a hit in the L2 cache need to send up to L1 cache
					//remove packet from l2 cache in queue
					message_packet->access_type = cgm_access_l2_store_reply;
					list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
					list_enqueue(l1_d_caches[my_pid].Rx_queue_top, message_packet);
					//cgm_cache_set_block(&(l2_caches[0]), *set_ptr, *way_ptr, tag, 1);

					future_advance(l1_d_cache, (etime.count + l1_d_caches[my_pid].wire_latency));
				}
				// L2 Cache Miss!
				else if(cache_status == 0)
				{
					//stats
					l2_caches[my_pid].misses++;

					//for now pretend that it is the last level of cache and memory ctrl.
					P_PAUSE(mem_miss);

					message_packet->access_type = cgm_access_l2_store_reply;

					cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, 4);

					list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
					list_enqueue(l1_d_caches[my_pid].Rx_queue_top, message_packet);

					future_advance(l1_d_cache, (etime.count + l1_d_caches[my_pid].wire_latency));
				}
			}
		//}

	return;
}*/
	
	
	
	
	
/*old code
		//while the next level of cache's in queue is full stall
		while(!cache_can_access_bottom(&l1_i_caches[cache->id]))
		{
			P_PAUSE(1);
		}

		//change access type, i cache only ever reads so puts is ok.
		message_packet->access_type = cgm_access_puts;

		//success, remove packet from l2 cache in queue
		list_remove(cache->last_queue, message_packet);

		CGM_DEBUG(GPU_cache_debug_file, "%s access_id %llu cycle %llu removed from %s size %d\n",
				cache->name, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));

		list_enqueue(l1_i_caches[cache->id].Rx_queue_bottom, message_packet);
		future_advance(&l1_i_cache[cache->id], WIRE_DELAY(l1_i_caches[cache->id].wire_latency));
	}
	else
	{
		fatal("l2_cache_access_retry(): miss on retry\n");
	}*/

/*void cpu_l1_cache_access_puts(struct cache_t *cache, struct cgm_packet_t *message_packet){

	struct cgm_packet_t *miss_status_packet;
	enum cgm_access_kind_t access_type;
	unsigned int addr = 0;
	long long access_id = 0;
	int set = 0;
	int tag = 0;
	unsigned int offset = 0;
	int way = 0;
	int state = 0;

	int *set_ptr = &set;
	int *tag_ptr = &tag;
	unsigned int *offset_ptr = &offset;
	int *way_ptr = &way;
	int *state_ptr = &state;

	int mshr_row = -1;

	int i = 0;

	//the packet is from the L2 cache
	access_type = message_packet->access_type;
	addr = message_packet->address;
	access_id = message_packet->access_id;

	//probe the address for set, tag, and offset.
	assert(addr != NULL);
	cgm_cache_decode_address(cache, addr, set_ptr, tag_ptr, offset_ptr);

	CGM_DEBUG(CPU_cache_debug_file, "%s access_id %llu cycle %llu puts\n", cache->name, access_id, P_TIME);

	//charge the delay for writing cache block
	cgm_cache_set_block(cache, *set_ptr, *way_ptr, *tag_ptr, cache_block_shared);
	P_PAUSE(cache->latency);

	//get the mshr status
	mshr_row = mshr_get_status(cache, set_ptr, tag_ptr, access_id);
	if(mshr_row == -1)
	{
		printf("%s crashing %llu access_id %llu type %s\n", cache->name, P_TIME, access_id, str_map_value(&cgm_mem_access_strn_map, message_packet->cpu_access_type));
		mshr_dump(cache);
		assert(mshr_row != -1);
	}

	//check the number of entries in the mshr row
	assert(list_count(cache->mshrs[mshr_row].entires) == cache->mshrs[mshr_row].num_entries);
	assert(cache->mshrs[mshr_row].num_entries > 0);

	CGM_DEBUG(mshr_debug_file, "%s access_id %llu cycle %llu mshr_row %d num_entries %d\n", cache->name, access_id, P_TIME, mshr_row, cache->mshrs[mshr_row].num_entries);

	//move them to the retry queue
	for(i = 0; i < cache->mshrs[mshr_row].num_entries; i++)
	{
		miss_status_packet = list_dequeue(cache->mshrs[mshr_row].entires);

		CGM_DEBUG(mshr_debug_file, "%s access_id %llu coalesced %d tag %d set %d\n",
				cache->name, miss_status_packet->access_id, miss_status_packet->coalesced, miss_status_packet->tag, miss_status_packet->set);

		assert(miss_status_packet != NULL);
		//assert(miss_status_packet->address != 0);

		if (miss_status_packet->access_id == access_id)
		{
			//this is the first entry and was not coalesced
			//assert(miss_status_packet->access_id == access_id);
			assert(miss_status_packet->coalesced == 0);

			//we can put either the message_packet or miss_status_packet in the retry queue.
			message_packet->access_type = cgm_access_retry;
			list_remove(cache->last_queue, message_packet);
			list_enqueue(cache->retry_queue, message_packet);

		}
		else
		{
			//this is a coalesced packet
			if(miss_status_packet->coalesced != 1)
			{
				printf("breaking access_id %llu cycle %llu\n", access_id, P_TIME);
				printf("i %d miss sp %llu, coalesced %d\n", i, miss_status_packet->access_id, miss_status_packet->coalesced);

				mshr_dump(cache);

				STOP;
			}

			assert(miss_status_packet->coalesced == 1);

			//drop it into the retry queue
			list_enqueue(cache->retry_queue, miss_status_packet);

		}
	}

	//advance myself by the number of packets.
	long long time = etime.count;  :-P
	for(i = 0; i < cache->mshrs[mshr_row].num_entries; i ++)
	{
		time += 2;
		future_advance(cache->ec_ptr, time);
	}

	//clear the mshr row for future use
	mshr_clear(&(cache->mshrs[mshr_row]));

	return;
}*/


//struct cgm_packet_status_t *mshr_remove(struct cache_t *cache);



/*struct cgm_packet_status_t *mshr_remove(struct cache_t *cache){

	int i = 0;
	struct cgm_packet_status_t *miss_status_packet;
	struct cgm_packet_status_t *message_packet;


	list_enqueue(cache->retry_queue, message_packet);



	LIST_FOR_EACH(cache->mshr, i)
	{
		mshr_packet = list_get(cache->mshr, i);

		if (mshr_packet->access_id == access_id)
		{
			return list_remove_at(cache->mshr, i);
		}
	}

	return NULL;
}*/



//this needs to be deleted
//int cache_get_state(struct cache_t *cache, enum cgm_access_kind_t access_type, int *tag_ptr, int *set_ptr, unsigned int *offset_ptr, int *way_ptr, int *state_ptr);


//void cpu_l1_cache_access_puts(struct cache_t *cache, struct cgm_packet_t *message_packet);

/*int cache_get_state(struct cache_t *cache, enum cgm_access_kind_t access_type, int *tag_ptr, int *set_ptr, unsigned int *offset_ptr, int *way_ptr, int *state_ptr){

	cache_block_invalid = 0
	cache_block_noncoherent = 1
	cache_block_modified = 2
	cache_block_owned = 3
	cache_block_exclusive = 4
	cache_block_shared = 5

	int cache_status;

	//stats


	//find the block in the cache and get it's state
	cache_status = cgm_cache_find_block(cache, tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);

	printf("cache_status %d\n", cache_status);
	getchar();

	//hit and state is M, E, or S we are done at this level of cache
	if((cache_status == 1 && *state_ptr == 2) || (cache_status == 1 && *state_ptr == 4 ) || (cache_status == 1 && *state_ptr == 5))
	{
		//stats
		cache->hits++;

		//done, respond to requester.
		return 1;
	}
	//hit and state is invalid (miss)
	else if(cache_status == 1 && *state_ptr == 0)
	{
		//stats
		cache->invalid_hits++;

		//treat this like a miss
		return 2;

	}
	//the cache block is not present m the cache (miss)
	else if(cache_status == 0)
	{
		//stats
		cache->misses++;

		return 3;
	}
	else if (cache_status == 1 && *state_ptr == 1)
	{
		printf("CRASHING cache_status %d state_ptr %d\n", cache_status, *state_ptr);
		getchar();
		fatal("cache_mesi_load() non cached state\n");
	}
	else
	{
		printf("CRASHING cache_status %d state_ptr %d\n", cache_status, *state_ptr);
		getchar();
		fatal("cache_mesi_load() something went wrong here\n");
	}

	return 0;

}*/


//fixed
	//===============================================
	/*if(mshr_status >= 0)
	{
		we have outstanding mshr requests so set the retry state bit
		*retry_ptr = cache->mshrs[mshr_status].num_entries;
		assert(*retry_ptr > 0);
	}

	//move the access and any coalesced accesses to the retry queue.
	for(i = 0; i < *retry_ptr; i++)
	{
		if( i == 0)
		{
			//move current message_packet to retry queue
			message_packet->access_type = cgm_access_retry;
			list_remove(cache->last_queue, message_packet);
			list_enqueue(cache->retry_queue, message_packet);
			advance(&l2_cache[cache->id]);
		}
		else if( i > 0)
		{
			miss_status_packet = list_remove_at(cache->mshrs[mshr_status].entires, i);
			miss_status_packet->coalesced_packet->access_type = cgm_access_retry;
			list_enqueue(cache->retry_queue, miss_status_packet->coalesced_packet);
			free(miss_status_packet);
			advance(&l2_cache[cache->id]);
		}
	}

	mshr_clear(&(cache->mshrs[mshr_status]));*/










//fixed
		/*if(mshr_status == 1)
		{
			//access is unique in the MSHR
			//while the next level's queue is full stall
			while(!switch_can_access(switches[cache->id].north_queue))
			{
				P_PAUSE(1);
			}

			CGM_DEBUG(cache_debug_file, "l2_cache[%d] access_id %llu cycle %llu miss switch north queue free size %d\n",
					cache->id, access_id, P_TIME, list_count(switches[cache->id].north_queue));

			//send to L3 cache over switching network add source and dest here
			//star todo send to correct l3 dest
			message_packet->access_type = cgm_access_gets_i;
			message_packet->src_name = cache->name;
			message_packet->source_id = str_map_string(&node_strn_map, cache->name);
			message_packet->dest_name = l3_caches[cache->id].name;
			message_packet->dest_id = str_map_string(&node_strn_map, l3_caches[cache->id].name);


			list_remove(cache->last_queue, message_packet);
			CGM_DEBUG(cache_debug_file, "l2_cache[%d] access_id %llu cycle %llu removed from %s size %d\n",
					cache->id, access_id, P_TIME, cache->last_queue->name, list_count(cache->last_queue));
			list_enqueue(switches[cache->id].north_queue, message_packet);

			future_advance(&switches_ec[cache->id], WIRE_DELAY(switches[cache->id].wire_latency));

			CGM_DEBUG(cache_debug_file, "l2_cache[%d] access_id %llu cycle %llu l2_cache[%d] as %s\n",
				cache->id, access_id, P_TIME, cache->id, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

			CGM_DEBUG(protocol_debug_file, "Access_id %llu cycle %llu l1_i_cache[%d] Miss\tSEND l2_cache[%d] -> %s\n",
				access_id, P_TIME, cache->id, cache->id, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

		}*/


//==================================


			/*//stats
			if(access_type == cgm_access_fetch)
				l1_i_caches[my_pid].fetches++;

			if(access_type == cgm_access_retry)
				l1_i_caches[my_pid].retries++;

			//memory access from CPU
			addr = message_packet->address;
			access_id = message_packet->access_id;

			//probe the address for set, tag, and offset.
			cgm_cache_decode_address(&(l1_i_caches[my_pid]), addr, set_ptr, tag_ptr, offset_ptr);

			CGM_DEBUG(cache_debug_file,"l1_i_cache[%d] access_id %llu cycle %llu as %s addr 0x%08u, tag %d, set %d, offset %u\n",
					my_pid, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, access_type), addr, *tag_ptr, *set_ptr, *offset_ptr);

			//get the block and the state of the block and charge a cycle
			cache_status = cgm_cache_find_block(&(l1_i_caches[my_pid]), tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
			P_PAUSE(2);

			//L1 I Cache Hit!
			if(cache_status == 1 && *state_ptr != 0)
			{
				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu hit\n", my_pid, access_id, P_TIME);

				if(access_type == cgm_access_retry)
					retry_ptr--;

				if(access_type == cgm_access_fetch)
					l1_i_caches[my_pid].hits++;

				//remove packet from cache queue, global queue, and simulator memory
				//note cycle already charged

				list_remove(l1_i_caches[my_pid].last_queue, message_packet);
				remove_from_global(access_id);
				free(message_packet);

			}
			//L1 I Cache Miss!
			else if(cache_status == 0 || *state_ptr == 0)
			{
				//all mshr based retires should be hits
				//star todo there is a bug here 1 access fails retry in our MM.
				assert(message_packet->access_type != cgm_access_retry);

				if(access_type == cgm_access_fetch)
					l1_i_caches[my_pid].misses++;

				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss\n", my_pid, access_id, P_TIME);

				miss_status_packet = miss_status_packet_create(message_packet->access_id, message_packet->access_type, set, tag, offset);
				mshr_status = mshr_set(&(l1_i_caches[my_pid]), miss_status_packet, message_packet);

				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss mshr status %d\n", my_pid, access_id, P_TIME, mshr_status);

				if(mshr_status == 1)
				{
					//access is unique in the MSHR
					//while the next level of cache's in queue is full stall
					while(!cache_can_access_top(&l2_caches[my_pid]))
					{
						P_PAUSE(1);
					}

					CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss l2 queue free\n", my_pid, access_id, P_TIME);

					change the access type for the coherence protocol and drop into the L2's queue
					remove the access from the l1 cache queue and place it in the l2 cache ctrl queue
					message_packet->access_type = cgm_access_gets_i;
					list_remove(l1_i_caches[my_pid].last_queue, message_packet);
					list_enqueue(l2_caches[my_pid].Rx_queue_top, message_packet);

					CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu l2_cache[%d] -> %s\n",
							my_pid, access_id, P_TIME, my_pid, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

					//advance the L2 cache adding some wire delay time.
					future_advance(&l2_cache[my_pid], (etime.count + (l2_caches[my_pid].wire_latency * 2)));
				}
				else if(mshr_status == 0)
				{
					//mshr is full so we can't progress, retry.
					message_packet->access_type = cgm_access_retry;
					future_advance(&l1_i_cache[my_pid], (etime.count + 2));

				}
				else
				{
					//access was coalesced. For now do nothing until later.
				}

				//done
			}
		}*/

		/*else if(access_type == cgm_access_puts)
		{
			//the packet is from the L2 cache
			addr = message_packet->address;
			access_id = message_packet->access_id;

			//probe the address for set, tag, and offset.
			cgm_cache_decode_address(&(l1_i_caches[my_pid]), addr, set_ptr, tag_ptr, offset_ptr);

			CGM_DEBUG(cache_debug_file, "l1_i_cache[%d] access_id %llu cycle %llu puts\n", my_pid, access_id, P_TIME);

			//charge the delay for writing cache block
			cgm_cache_set_block(&l1_i_caches[my_pid], *set_ptr, *way_ptr, tag, cache_block_shared);
			P_PAUSE(1);

			//get the mshr status
			mshr_status = mshr_get(&l1_i_caches[my_pid], set_ptr, tag_ptr);
			assert(mshr_status != -1);

			if(mshr_status >= 0)
			{
				we have outstanding mshr requests so set the retry state bit
				*retry_ptr = l1_i_caches[my_pid].mshrs[mshr_status].num_entries;
				//printf("retry_ptr %d\n", *retry_ptr);
				assert(*retry_ptr > 0);
			}

			advance_time = etime.count + 2;

			//move the access and any coalesced accesses to the retry queue.
			for(i = 0; i < *retry_ptr; i++)
			{
				if( i == 0)
				{
					//move current message_packet to retry queue
					message_packet->access_type = cgm_access_retry;
					list_remove(l1_i_caches[my_pid].next_queue, message_packet);
					list_enqueue(l1_i_caches[my_pid].retry_queue, message_packet);

					//printf("list count %d\n", list_count(l1_i_caches[my_pid].retry_queue));

					advance(&l1_i_cache[my_pid]);
				}
				else if( i > 0)
				{
					miss_status_packet = list_remove_at(l1_i_caches[my_pid].mshrs[mshr_status].entires, i);
					list_enqueue(l1_i_caches[my_pid].retry_queue, miss_status_packet->coalesced_packet);
					free(miss_status_packet);
					advance_time += 2;
					advance(&l1_i_cache[my_pid]);
				}
			}

			//clear the mshr row for future use
			mshr_clear(&l1_i_caches[my_pid].mshrs[mshr_status]);
			//done.
		}


	}*/


else if(cache_status == 0 || *state_ptr == 0)
{

				// L2 Cache Miss!
				l2_caches[my_pid].misses++;

				/*printf("access id %llu l1 miss\n", access_id);
				getchar();*/


				//star todo check on size of MSHR
				mshr_packet = status_packet_create();

				//drop a token in the mshr queue
				//star todo add some detail to this so we can include coalescing
				//star todo have an MSHR hit advance the cache and clear out the request.
				mshr_packet->access_type = message_packet->access_type;
				mshr_packet->access_id = message_packet->access_id;
				mshr_packet->in_flight = message_packet->in_flight;
				list_enqueue(l2_caches[my_pid].mshr, mshr_packet);


				message_packet->access_type = cgm_access_puts;

				//set the block now for testing///////////
				cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, cache_block_shared);
				///////////////////////////////////////////

				list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
				list_enqueue(l1_i_caches[my_pid].Rx_queue_top, message_packet);

				future_advance(&l1_i_cache[my_pid], (etime.count + l1_i_caches[my_pid].wire_latency));
			}
			
			
			
//printf("After probe addr 0x%08x\n", addr);

	/*printf("cache->log_block_size = %d\n",cache->log_block_size);
	printf("cache->block_mask %d\n", cache->block_mask);
	printf("\n");*/
	
	//notes this is useing the tag and indx to calculate set location.

	/*printf("---set_ptr---\n");
	printf("Addr 0x%08x\n", addr);
	printf("(addr >> cache->log_block_size) = 0x%08x\n", addr >> cache->log_block_size);
	printf("set_ptr %d\n", (addr >> cache->log_block_size) % cache->num_sets);
	printf("---set_ptr---\n");*/

	/*printf("---tag_ptr---\n");
	printf("Addr 0x%08X\n", addr);
	printf("~(cache->block_mask) 0x%08x\n", ~(cache->block_mask));
	printf("addr & ~(cache->block_mask) 0x%08x\n", addr & ~(cache->block_mask));
	printf("tag %d\n", *tag_ptr);
	printf("---tag_ptr---\n");
	getchar();*/

	/*printf("---offset_ptr---\n");
	printf("Addr 0x%08x\n", addr);
	printf("(cache->block_mask) 0x%08x\n", (cache->block_mask));
	printf("addr & (cache->block_mask) 0x%08x\n", addr & addr & (cache->block_mask));
	printf("---offset_ptr---\n");
	getchar();*/
	
	