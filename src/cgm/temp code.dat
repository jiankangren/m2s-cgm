//==================================


			/*//stats
			if(access_type == cgm_access_fetch)
				l1_i_caches[my_pid].fetches++;

			if(access_type == cgm_access_retry)
				l1_i_caches[my_pid].retries++;

			//memory access from CPU
			addr = message_packet->address;
			access_id = message_packet->access_id;

			//probe the address for set, tag, and offset.
			cgm_cache_decode_address(&(l1_i_caches[my_pid]), addr, set_ptr, tag_ptr, offset_ptr);

			CGM_DEBUG(cache_debug_file,"l1_i_cache[%d] access_id %llu cycle %llu as %s addr 0x%08u, tag %d, set %d, offset %u\n",
					my_pid, access_id, P_TIME, (char *)str_map_value(&cgm_mem_access_strn_map, access_type), addr, *tag_ptr, *set_ptr, *offset_ptr);

			//get the block and the state of the block and charge a cycle
			cache_status = cgm_cache_find_block(&(l1_i_caches[my_pid]), tag_ptr, set_ptr, offset_ptr, way_ptr, state_ptr);
			P_PAUSE(2);

			//L1 I Cache Hit!
			if(cache_status == 1 && *state_ptr != 0)
			{
				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu hit\n", my_pid, access_id, P_TIME);

				if(access_type == cgm_access_retry)
					retry_ptr--;

				if(access_type == cgm_access_fetch)
					l1_i_caches[my_pid].hits++;

				//remove packet from cache queue, global queue, and simulator memory
				//note cycle already charged

				list_remove(l1_i_caches[my_pid].last_queue, message_packet);
				remove_from_global(access_id);
				free(message_packet);

			}
			//L1 I Cache Miss!
			else if(cache_status == 0 || *state_ptr == 0)
			{
				//all mshr based retires should be hits
				//star todo there is a bug here 1 access fails retry in our MM.
				assert(message_packet->access_type != cgm_access_retry);

				if(access_type == cgm_access_fetch)
					l1_i_caches[my_pid].misses++;

				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss\n", my_pid, access_id, P_TIME);

				miss_status_packet = miss_status_packet_create(message_packet->access_id, message_packet->access_type, set, tag, offset);
				mshr_status = mshr_set(&(l1_i_caches[my_pid]), miss_status_packet, message_packet);

				CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss mshr status %d\n", my_pid, access_id, P_TIME, mshr_status);

				if(mshr_status == 1)
				{
					//access is unique in the MSHR
					//while the next level of cache's in queue is full stall
					while(!cache_can_access_top(&l2_caches[my_pid]))
					{
						P_PAUSE(1);
					}

					CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu miss l2 queue free\n", my_pid, access_id, P_TIME);

					change the access type for the coherence protocol and drop into the L2's queue
					remove the access from the l1 cache queue and place it in the l2 cache ctrl queue
					message_packet->access_type = cgm_access_gets_i;
					list_remove(l1_i_caches[my_pid].last_queue, message_packet);
					list_enqueue(l2_caches[my_pid].Rx_queue_top, message_packet);

					CGM_DEBUG(cache_debug_file, "\tl1_i_cache[%d] access_id %llu cycle %llu l2_cache[%d] -> %s\n",
							my_pid, access_id, P_TIME, my_pid, (char *)str_map_value(&cgm_mem_access_strn_map, message_packet->access_type));

					//advance the L2 cache adding some wire delay time.
					future_advance(&l2_cache[my_pid], (etime.count + (l2_caches[my_pid].wire_latency * 2)));
				}
				else if(mshr_status == 0)
				{
					//mshr is full so we can't progress, retry.
					message_packet->access_type = cgm_access_retry;
					future_advance(&l1_i_cache[my_pid], (etime.count + 2));

				}
				else
				{
					//access was coalesced. For now do nothing until later.
				}

				//done
			}
		}*/

		/*else if(access_type == cgm_access_puts)
		{
			//the packet is from the L2 cache
			addr = message_packet->address;
			access_id = message_packet->access_id;

			//probe the address for set, tag, and offset.
			cgm_cache_decode_address(&(l1_i_caches[my_pid]), addr, set_ptr, tag_ptr, offset_ptr);

			CGM_DEBUG(cache_debug_file, "l1_i_cache[%d] access_id %llu cycle %llu puts\n", my_pid, access_id, P_TIME);

			//charge the delay for writing cache block
			cgm_cache_set_block(&l1_i_caches[my_pid], *set_ptr, *way_ptr, tag, cache_block_shared);
			P_PAUSE(1);

			//get the mshr status
			mshr_status = mshr_get(&l1_i_caches[my_pid], set_ptr, tag_ptr);
			assert(mshr_status != -1);

			if(mshr_status >= 0)
			{
				we have outstanding mshr requests so set the retry state bit
				*retry_ptr = l1_i_caches[my_pid].mshrs[mshr_status].num_entries;
				//printf("retry_ptr %d\n", *retry_ptr);
				assert(*retry_ptr > 0);
			}

			advance_time = etime.count + 2;

			//move the access and any coalesced accesses to the retry queue.
			for(i = 0; i < *retry_ptr; i++)
			{
				if( i == 0)
				{
					//move current message_packet to retry queue
					message_packet->access_type = cgm_access_retry;
					list_remove(l1_i_caches[my_pid].next_queue, message_packet);
					list_enqueue(l1_i_caches[my_pid].retry_queue, message_packet);

					//printf("list count %d\n", list_count(l1_i_caches[my_pid].retry_queue));

					advance(&l1_i_cache[my_pid]);
				}
				else if( i > 0)
				{
					miss_status_packet = list_remove_at(l1_i_caches[my_pid].mshrs[mshr_status].entires, i);
					list_enqueue(l1_i_caches[my_pid].retry_queue, miss_status_packet->coalesced_packet);
					free(miss_status_packet);
					advance_time += 2;
					advance(&l1_i_cache[my_pid]);
				}
			}

			//clear the mshr row for future use
			mshr_clear(&l1_i_caches[my_pid].mshrs[mshr_status]);
			//done.
		}


	}*/


else if(cache_status == 0 || *state_ptr == 0)
{

				// L2 Cache Miss!
				l2_caches[my_pid].misses++;

				/*printf("access id %llu l1 miss\n", access_id);
				getchar();*/


				//star todo check on size of MSHR
				mshr_packet = status_packet_create();

				//drop a token in the mshr queue
				//star todo add some detail to this so we can include coalescing
				//star todo have an MSHR hit advance the cache and clear out the request.
				mshr_packet->access_type = message_packet->access_type;
				mshr_packet->access_id = message_packet->access_id;
				mshr_packet->in_flight = message_packet->in_flight;
				list_enqueue(l2_caches[my_pid].mshr, mshr_packet);


				message_packet->access_type = cgm_access_puts;

				//set the block now for testing///////////
				cgm_cache_set_block(&(l2_caches[my_pid]), *set_ptr, *way_ptr, tag, cache_block_shared);
				///////////////////////////////////////////

				list_remove(l2_caches[my_pid].Rx_queue_top, message_packet);
				list_enqueue(l1_i_caches[my_pid].Rx_queue_top, message_packet);

				future_advance(&l1_i_cache[my_pid], (etime.count + l1_i_caches[my_pid].wire_latency));
			}
			
			
			
//printf("After probe addr 0x%08x\n", addr);

	/*printf("cache->log_block_size = %d\n",cache->log_block_size);
	printf("cache->block_mask %d\n", cache->block_mask);
	printf("\n");*/
	
	//notes this is useing the tag and indx to calculate set location.

	/*printf("---set_ptr---\n");
	printf("Addr 0x%08x\n", addr);
	printf("(addr >> cache->log_block_size) = 0x%08x\n", addr >> cache->log_block_size);
	printf("set_ptr %d\n", (addr >> cache->log_block_size) % cache->num_sets);
	printf("---set_ptr---\n");*/

	/*printf("---tag_ptr---\n");
	printf("Addr 0x%08X\n", addr);
	printf("~(cache->block_mask) 0x%08x\n", ~(cache->block_mask));
	printf("addr & ~(cache->block_mask) 0x%08x\n", addr & ~(cache->block_mask));
	printf("tag %d\n", *tag_ptr);
	printf("---tag_ptr---\n");
	getchar();*/

	/*printf("---offset_ptr---\n");
	printf("Addr 0x%08x\n", addr);
	printf("(cache->block_mask) 0x%08x\n", (cache->block_mask));
	printf("addr & (cache->block_mask) 0x%08x\n", addr & addr & (cache->block_mask));
	printf("---offset_ptr---\n");
	getchar();*/
	
	